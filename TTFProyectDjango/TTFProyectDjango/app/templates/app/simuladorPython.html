<!DOCTYPE html>

<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>{% block title %}NETTLAERER{% endblock %}</title>

    <!--Archivos navbar-->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css"
          integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

    <!--Archivos pyscript-->
    <link rel="stylesheet" href="https://pyscript.net/latest/pyscript.css" />
    <script defer src="https://pyscript.net/latest/pyscript.js"></script>

    <!-- Add CSS style-->
    <link href="//fonts.googleapis.com/css?family=Lobster&subset=latin,latin-ext" rel="stylesheet" type="text/css">

    <!-- Configuracion PyScript -->
    <py-config>
        packages = [
        "pandas",
        "seaborn",
        "matplotlib",
        "scipy",
        "scikit-learn",
        ]
        terminal = false
        docked = false
    </py-config>

    <style>

        .header {
            padding-left: 50px;
            margin-top: 50px;
        }

        .menuBotones {
            text-align: center;
            padding: 50px;
        }

        .informacion {
            padding: 50px;
            text-align: left
        }

        .pyscript {
            padding-left: 50px;
            margin-top: 50px;
        }

        p {
            font-family: 'Lobster';
            text-align: left
        }

        table {
            margin-top: 30px;
            border-collapse: collapse;
        }
    </style>
</head>
<body>
    <!--Add a navbar header-->
    <div class="header">
        <h1><em>Menu Principal</em></h1>
        <nav class="navbar navbar-expand-lg navbar-light ">

            <ul class="nav">
                <li><a href="/" class="navbar-brand">NETTLAERER</a></li>
                <li>
                    <a href="{% url 'tecnologias' %}" class="navbar-brand">TECNOLOGIAS</a>
                </li>
                <li><a href="{% url 'MineriaDatos' %}" class="navbar-brand">Mineria de Datos</a></li>
            </ul>
        </nav>
    </div>

    <!--Mostrar informacion de interes-->
    <div class="informacion">   
        
        <!--Mostramos codigo-->
        <div id="codigoPython" style="display:none">
            <!--Python Packages-->
            <pre>
                
                import seaborn as sns
                import matplotlib.pyplot as plt
                from sklearn.model_selection import cross_val_score
                from sklearn.tree import DecisionTreeClassifier
                from sklearn.ensemble import RandomForestClassifier
                from sklearn.neighbors import KNeighborsClassifier
                from sklearn.svm import SVC
                from sklearn.metrics import confusion_matrix

                def showTitanicSurvived(df):
                    """ Generate the survived graphic of dataset titanic
                    :param df: Dataset
                    """
                    sns.countplot( x='Survived', data=df)
                    plt.title('Titanic Class')
                    display(plt)
                    plt.clf()

                def ClearLostDataTitanic(df) -> pd.DataFrame:
                    """ Clear lost data of dataset titanic
                    :param: Dataset
                    :return: Dataset
                    """
                    df = df.interpolate(method ='linear', limit_direction ='backward')
                    return df

                def transformSurvivedTitanic(df) -> pd.DataFrame:
                    """ Transform col Survived of dataset titanic and show a graphic of data distribution
                    :param df: Dataframe
                    :return df
                    """
                    df['Survived'] = df['Survived'].replace([1,0],['Alive','Dead'])
                    return df

                def Classifiers(train_data, test_data):
                    """ Apply classifier of mining data to dataset
                    :param train_data: Train Data of Dataset
                    :param test_data: Test data of Dataset
                    """
                    # Create models
                    models = [DecisionTreeClassifier(), KNeighborsClassifier(n_neighbors=3), RandomForestClassifier(), SVC(probability = True)]
                    classifiers = ['DecisionTree', 'KNN', 'RandomForest', 'SVC']
                    log_cols = ["Classifier", "Accuracy"]
                    log = pd.DataFrame(columns=log_cols)
                    acc_dict = {}
                    # train and test
                    i=0
                    for model in models:
                        name = model.__class__.__name__
                        acc = cross_val_score(model, train_data, test_data, cv=5, scoring='accuracy').mean()
                        log.loc[i, log_cols[0]] = models[i]
                        log.loc[i, log_cols[1]] = acc
                        i += 1
                    # Show result
                    display(log)
                    
                    plt.xlabel('Classifiers')
                    plt.title('Classifier Accuracy')
                    plt.figsize(15)
                    sns.barplot(x=classifiers, y=log.Accuracy, data=log, color='b')
                    display(plt)
                    plt.clf()

                def showTitanicAgeSexSurvived(df):
                    """ Analysis of the survivors according to the age and Sex of titanic dataset
                    :param df: Dataset titanic
                    """
                    df['Sex'] = df['Sex'].replace([0,1],['Female','Male'])
                    df.loc[ df['Age'] == 0, 'Age' ] = 'Young'
                    df.loc[ df['Age'] == 1, 'Age' ] = 'Adult'
                    df.loc[ df['Age'] == 2, 'Age' ] = 'Elderly'
                    sns.catplot(x='Age', hue='Survived', kind='count', col='Sex', data = df)
                    display(plt)
                    plt.clf()

                def showTitanicFarePclassSurvived(df):
                    """ Analysis of the survivors according to the fare and pclass of titanic dataset
                    :param df: Dataset titanic
                    """
                    sns.catplot(x='Fare', hue='Survived', kind='count', col='Pclass', data=df)
                    display(plt)
                    plt.clf()

                def showTitanicMembersFamilySurvived(df):
                    """ Analysis of the survivors according to the member family of titanic dataset
                    :param df: Dataset titanic
                    """
                    sns.catplot(x='Members_Family', hue='Survived', kind='count', data=df)
                    display(plt)
                    plt.clf()

                def generate_confusion_matrix(train,test,classifier):
                    """ Generate a confusion matrix
                    :param train: Data train
                    :param test: Data test
                    :classifier: Mining Data Classifier
                    """
                    classifier.fit(train,test)
                    cm = confusion_matrix(test, classifier.predict(train))
                    display(cm)

            </pre>
        </div>
    </div>
    
    <!--Contenedor de fluido-->
    <div class="container-fluid">
        <div class="row">
            <!--Añadimos un menu de filtro-->
            <div class="col-sm-3" style="">
                <em>Etapas del proceso de mineria de datos</em>
                <button onclick="$('#ObtencionDatos').show(); $('#PreprocesamientoDatos').hide(), $('#EjerciciosPropuestos').hide(), $('#Algoritmos').hide()">Paso 1. Obtención de Datos</button>
                <button onclick="$('#PreprocesamientoDatos').show();$('#ObtencionDatos').hide(), $('#EjerciciosPropuestos').hide(), $('#Algoritmos').hide()">Paso 2. Preprocesamiento de Datos</button>
                <button onclick="$('#Algoritmos').show(),$('#PreprocesamientoDatos').hide();$('#ObtencionDatos').hide(), $('#EjerciciosPropuestos').hide()">Paso 3. Algoritmos</button>
                <button onclick="$('#EjerciciosPropuestos').show(), $('#PreprocesamientoDatos').hide(), $('#ObtencionDatos').hide(),$('#Algoritmos').hide()">Ejercicios propuestos</button>
            </div>
            
            <div class="col-sm-10">
                <!--Paso 1: Obtencion de datos-->
                <div id="ObtencionDatos" style="display:none">
                    <!--PASO 1: OBTENCION DE DATOS-->
                    <h1>Paso 1: Extracción y lectura de datos</h1>
                    <p>
                        Antes de que podamos analizar los datos, dichos datos deben ser importados o extraídos. <br />
                        A continuación, veremos distintos ejemplos sobre cómo cargar e importar datos a partir de archivos, en formato csv, arff y data, mediante un enlace web.
                        <h2>Archivos CSV</h2>
                        En los ejemplos a continuación, veremos cómo obtener información a partir de archivos en formato csv.
                        <br />
                        Nota: Para obtener los conjuntos de datos, debemos usar la librería Pandas y pyodide.http; sin embargo, antes de obtener dichos datos debemos comprobar si el conjunto de datos se encuentra etiquetado o no etiquetado.<br />
                        <ul>
                            <li>Conjunto de datos etiquetado: Conjunto de datos que incluye las etiquetas de las columnas.</li>
                            <li>Conjunto de datos no etiqueta: Conjunto de datos que no incluye las etiquetas de las columnas.</li>
                        </ul>
                        <h3>Conjunto de datos etiquetado</h3>
                        En esta sección, obtendremos el conjunto de datos titanic. Dicho conjunto de datos contiene información sobre los pasajeros del titanic quienes fueron víctimas
                        de los naufragios más infames de la historia.
                        <br />Dicho conjunto de datos aporta la siguiente información:
                        <ul>
                            <li>PassengerId: Representa el identificador del pasajero</li>
                            <li>Survived: Representa la supervivencia del pasajero (0=No, 1=Sí).</li>
                            <li>Pclass: Representa la clase del ticket (1=Primero, 2=Segundo, 3=Tercero).</li>
                            <li>Name: Representa el nombre del pasajero</li>
                            <li>Sex: Representa el sexo del pasajero (male=Masculino, female=Femenino)</li>
                            <li>Age: Representa la edad del pasajero, en años</li>
                            <li>SibSp: Representa el número de hermanos o cónyuges a bordo del Titanic</li>
                            <li>Parch: Representa el número de padres o hijos a bordo del Titanic</li>
                            <li>Ticket: Representa el número del ticket</li>
                            <li>Fare: Representa la tarifa del pasajero</li>
                            <li>Cabin: Representa el número de la cabina</li>
                            <li>Embarked: Representa el puerto de embarque</li>
                        </ul>
                        En el código a continuación, veremos como obtener el conjunto de datos titanic a partir del enlace web:
                        <py-repl>
                            import pandas as pd
                            from pyodide.http import open_url
                            titanic = pd.read_csv(open_url("https://raw.githubusercontent.com/p72losaj/Datasets/main/titanic.csv"),header=0, sep=",")
                            display(titanic)
                        </py-repl>
                        <em>Explicación del ejemplo</em>
                        <ul>
                            <li>Importamos las librerías pandas y pyodide.http</li>
                            <li>Nombramos el dataFrame como titanic</li>
                            <li>header=0 significa que las cabeceras de los nombres de las variables se encuentran en la primera fila.</li>
                            <li>sep="," significa que "," es usado como el separador entre las variables</li>
                        </ul>
                        Consejo: Si obtenemos un conjunto de datos con un número elevado de filas, podemos usar la función head(x) para mostrar las primeras x filas del conjunto de datos.
                        <br />Si no especificamos un valor de x, por defecto se muestran las primeras 5 filas del conjunto de datos.
                        <br />En el código a continuación, veremos como mostrar las primeras 5 filas de un conjunto de datos:
                        <py-repl>
                            import pandas as pd
                            from pyodide.http import open_url
                            titanic = pd.read_csv(open_url("https://raw.githubusercontent.com/p72losaj/Datasets/main/titanic.csv"),
                            header=0,
                            sep=",")
                            display(titanic.head())
                        </py-repl>
                        <h3>Conjunto de datos no etiquetado</h3>
                        Para esta sección, se obtendrá el conjunto de datos iris. El conjunto de datos iris contiene información de un estudio de 3 especies de Iris (Iris setosa, Iris Virginica e Iris versicolor).
                        <br />Dicho conjunto de datos aporta la siguiente información:
                        <ul>
                            <li>sepal length: Representa la longitud del sépalo, en centímetros.</li>
                            <li>sepal width: Representa la anchura del sépalo, en centímetros.</li>
                            <li>petal length: Representa la longitud del pétalo, en centímetros.</li>
                            <li>petal width: Representa la anchura del pétalo, en centímetros.</li>
                            <li>Species: Representa las especies de Iris</li>
                        </ul>
                        <br />En el código a continuación, veremos un método para obtener dicho conjunto de datos.
                        <py-repl>
                            import pandas as pd
                            from pyodide.http import open_url
                            iris = pd.read_csv(open_url("https://raw.githubusercontent.com/p72losaj/Datasets/main/iris.data"),
                            header=None,
                            sep=",",
                            names = ["sepal length","sepal width","petal length","petal width","Species"])
                            display(iris.head(7))
                        </py-repl>
                        <em>Explicación del ejemplo</em>
                        <ul>
                            <li>Importamos las librerías pandas y pyodide.http</li>
                            <li>Nombramos el dataFrame como titanic</li>
                            <li>header=1 significa que las cabeceras de los nombres de las variables no se encuentran en la primera fila.</li>
                            <li>sep="," significa que "," es usado como el separador entre las variables</li>
                            <li>names = ["sepal length","sepal width","petal length","petal width","Species"] significa la secuencia de etiquetado de las columnas del conjunto de datos a aplicar.</li>
                        </ul>
                    </p>

                    <h2>Archivos ARFF</h2>
                    <p>
                        En esta sección, obtendremos el conjunto de datos segment_challenge. Al ser un archivo arff, toda la información referente al mismo se visualiza en el propio documento.
                        <br />En el código a continuación, veremos como importar los datos de dicho conjunto de datos.
                        <py-repl>
                            import pandas as pd
                            from pyodide.http import open_url
                            from scipy.io import arff
                            segment_challenge = pd.DataFrame(arff.loadarff(open_url("https://raw.githubusercontent.com/p72losaj/Datasets/main/segment-challenge.arff"))[0])
                            display(segment_challenge.head(8))
                        </py-repl>
                        <em>Explicación del ejemplo</em>
                        <ul>
                            <li>Importamos las librerías pandas, pyodide y scipy.io</li>
                            <li>Nombramos el dataframe como segment_challenge</li>
                            <li>Usamos la funcion arff.loadarff para cargar el archivo arff.</li>
                            <li>Usamos la función DataFrame para transformar el archivo arff en un dataframe de pandas</li>
                        </ul>
                        Si visualizamos la información obtenida y la comparamos con la sección data del archivo, podemos observar que se ha producido un error de traducción en la columna class del conjunto de datos
                        <br />Para corregir dicho error de codificación, emplearemos la función decode, tal y como vemos en el código a continuación:
                        <py-repl>
                            segment_challenge['class'] = segment_challenge['class'].str.decode('utf-8')
                            display(segment_challenge['class'].head(10))
                        </py-repl>
                    </p>

                    <h2>Archivos Data</h2>
                    <p>
                        En esta sección, obtendremos el conjunto de datos balance scale. Este conjunto de datos se generó para modelar resultados de experimentos psicológicos.
                        <br />La información que aporta dicho conjunto de datos es la siguiente:
                        <ul>
                            <li>class: Representa el atributo de clase del conjunto de datos</li>
                            <li>left_weight: Representa el peso a la izquierda de la balanza.</li>
                            <li>left_distance: Representa la distancia a la izquierda de la balanza.</li>
                            <li>right_weight: Representa el peso a la derecha de la balanza.</li>
                            <li>right_distance: Representa la distancia a la derecha de la balanza.</li>
                        </ul>
                        En el código a continuación, veremos como obtener la información del conjunto de datos:
                        <h4>Ejemplo 6</h4>
                        <py-repl>
                            import pandas as pd
                            from pyodide.http import open_url
                            balance_scale = pd.read_csv(open_url("https://raw.githubusercontent.com/p72losaj/Datasets/main/balance-scale.data"),
                                header=None,
                                sep=",",
                                names = ['class', 'left_weight', 'left_distance', 'right_weight', 'right_distance'])
                            display(balance_scale.head())
                        </py-repl>
                    </p>

</div>
                <!--Paso 2: Preprocesamiento de datos-->
                <div id="PreprocesamientoDatos" style="display:none">
                    <h1>Paso 2. Preprocesamiento de Datos</h1>
                    <p>
                        Una vez importados/extraídos los datos, es importante preprocesar los datos como un paso previo al análisis de los mismos. Para realizar el preprocesamiento de los datos, es importante establecer la meta final de nuestro análisis de los datos. 
                        <br />
                        Recomendación: Antes de comenzar el preprocesamiento de datos, es recomendable establecer la meta final de nuestro análisis debido a que todo este proceso dependerá de dicha meta. <br />
                        Esta meta no es única, es decir, dependerá del conjunto de datos y de la decisión de cada persona.
                        <br />
                    En esta sección, veremos los siguientes pasos para realizar el preprocesamiento de los datos:
                    <ol>
                        <li>Evaluación de los datos: Llevar a cabo una evaluación de la calidad de los datos ayuda a establecer la solidez de los mismos.
                        <li>Limpieza de los datos: La limpieza de los datos tiene como objetivo crear conjuntos de datos sencillos y completos para que los programas ejecuten el análisis.</li>
                        <li>Integrar y transformar los datos: Se utiliza la transformación para convertir los datos en formatos adecuados que el software informático y el aprendizaje automático puedan leer e interpretar.</li>
                    </ol>
                    <p>
                        Al evaluar los datos, es recomendable la realización de una búsqueda de los siguientes datos:
                        <ul>
                            <li>
                                Datos ruidosos, se refiere a aquellos datos sin valor significativo, como las entradas duplicadas o los campos de datos no relevantes para su análisis.
                                <br />La eliminación de los datos innecesarios ayuda a analizar los datos así como disminuir el tiempo de entrenamiento y test de los algoritmos que vayamos a desarrollar, como por ejemplo, árbol de decisión o Regresión Logística.
                            </li>
                            <li>
                                Datos perdidos o faltantes, se refiere a toda pérdida de información por diversas causas como por ejemplo un error humano, un mal funcionamiento u otros factores.
                                <br />La sustitución de estos datos ayuda a garantizar que los futuros análisis sean precisos y fiables.
                            </li>
                            <li>
                                Datos atípicos, se refiere a aquellos datos extraños en el conjunto de datos.
                            </li>
                            <li>
                                Categorías de datos, se refiere a los tipos de datos que componen el conjunto de datos. Los datos pueden ser divididos en 3 categorías: Numéricos, Categóricos u Ordinales.
                            </li>
                        </ul>
                        <h2>El conjunto de datos</h2>
                        Para este tutorial vamos a trabajar con el conjunto de datos titanic. Primero, vamos a obtener el conjunto de datos. Una vez obtenido el conjunto de datos, la meta que vamos a establecer es comprobar quienes fueron los pasajeros del titanic con mayores posibilidades de supervivencia.
                        <py-repl>
                            import pandas as pd
                            from pyodide.http import open_url
                            titanic = pd.read_csv(open_url("https://raw.githubusercontent.com/p72losaj/Datasets/main/titanic.csv"),header=0, sep=",")
                            display(titanic)
                        </py-repl>

                        <h2>Limpieza de Datos</h2>
                        A continuación, analizaremos el dataset titanic y aplicaremos distintas técnicas de detección y limpieza de datos.
                        <h3>Información innecesaria</h3>
                        Si observamos el dataset titanic comprobamos que para la meta establecida la información referida tanto al PassengerId, al ticket del pasajero (Ticket) y al puerto de embarque (Embarked) no aporta información relevante, de modo que podemos eliminar dicha información.
                        <br />Además, consideraremos que la columna Name no aporta la suficiente información relevante para nuestra meta.
                        <br /> Para eliminar las columnas del dataset, emplearemos la función drop de Pamdas, tal y como veremos en el código a continuación:
                        <br />Nota: Es recomendable que al modificar un dataset, realicemos una copia del mismo y modifiquemos dicha copia.
                        <py-repl>
                            titanic_copia = titanic.copy()
                            titanic_copia.drop(['PassengerId','Name','Ticket','Embarked'], axis=1, inplace=True)
                            display(titanic_copia.head())
                        </py-repl>
                        <em>Explicación:</em>
                        <ul>
                            <li>Empleamos la función copy() de Pandas para realizar una copia completa del dataset titanic y almacenamos dicha copia en la variable titanic_copia</li>
                            <li>Para eliminar la columna Ticket, empleamos la función drop de Pandas.</li>
                            <li>axis=1 significa que se elimina la columna completa</li>
                            <li>inplace=True significa que se aplican los cambios directamente sobre el conjunto de datos titanic_copia</li>
                        </ul>
                        <h3>Datos atípicos</h3>
                        Existen diversas formas de detectar datos atípicos, los cuáles abarcan desde generar gráficas hasta el uso de métodos matemáticos.
                        <h4>Diagrama de cajas</h4>
                        <br />Una forma simple y efectiva de visualizar datos atípicos, y resulta prácticamente útil para buscar valores atípicos, es generar diagramas de caja. Para generar estos diagramas, emplearemos la función boxplot de la librería seaborn.
                        <br />Sin embargo, cabe destacar que es recomendable para garantizar una correcta detección de dichos datos, generar dichos gráficos para columnas individuales del conjunto de datos siempre y cuando el conjunto de datos disponga de pocas columnas.
                        <br />En el código a continuación, veremos como generar los diagramas de caja:
                        <py-repl>
                            import seaborn as sns
                            import matplotlib.pyplot as plt
                            sns.boxplot(titanic_copia['Age'])
                            display(plt)
                            # Limpiamos la gráfica generada usando la función clf de matplotlib.pyplot
                            plt.clf()
                        </py-repl>
                        En el gráfico generado, podemos observar que los valores que se encuentren fuera del diagrama se consideran valores atípicos. Al final veremos como eliminar los datos atípicos del conjunto de datos.
                        Es necesario indicar que los diagramas de caja se pueden usar cuando tenemos datos en una sola dimensión.
                        <h4>Rango intercuartílico</h4>
                        Un método usado con mucha frecuencia en la investigación para limpiar datos mediante la eliminación de valores atípicos es calculando el rango intercuartílico para detectar dichos valores.
                        <br />El rango intercuartílico (IQR) es la diferencia entre el tercer cuantil (Q3) y el primer cuantil (Q1). Para calcular dichos valores, el método más sencillo es usando la función describe() de Pandas
                        <py-repl>
                            display(titanic_copia.describe())
                        </py-repl>
                        Si observamos la tabla obtenida, podemos obtener la siguiente información para cada columna del conjunto de datos:
                        <ul>
                            <li>count, significa el número de elementos de la columna.</li>
                            <li>mean, representa el valor medio obtenido a partir de los datos de la columna.</li>
                            <li>std, representa el valor de la desviación típica obtenido a partir de los datos de la columna.</li>
                            <li>min, representa el valor mínimo de la columna.</li>
                            <li>25%, representa el valor del primer percentil (Q1).</li>
                            <li>50%, representa el valor de la mediana obtenido a partir de los datos de la columna.</li>
                            <li>75%, representa el valor del tercer percentil (Q3).</li>
                            <li>max, representa el valor máximo de la columna.</li>
                        </ul>
                        Con los datos de los percentiles Q1 y Q3, podemos definir nuestros límites inferior y superior. Es decir, cualquier punto por debajo del límite inferior y por encima del
                        límite superior se considerará un valor atípico. En nuestro caso, emplearemos este método.
                        <br />En el código a continuación mostraremos cómo detectar y eliminar valores atípicos usando el rango intercuartílico, para la columna Age. Para ello, usaremos la librería numpy para obtener los índices de los valores atípicos:
                        <py-repl>
                            import numpy as np
                            # Mostramos el numero de filas y columnas del conjunto de datos con valores atípicos
                            display('Filas,columnas con valores atipicos: ', titanic_copia.shape)
                            # Calculamos el valor de IQR
                            Q1 = 20.125000; Q3 = 38.000000; IQR = Q3 - Q1
                            # Definimos el límite superior e inferior
                            upper = np.where(titanic_copia['Age'] >= (Q3+1.5*IQR))
                            lower = np.where(titanic_copia['Age'] <= (Q1-1.5*IQR))
                            # Removemos los valores atípicos
                            titanic_copia.drop(upper[0], inplace = True)
                            titanic_copia.drop(lower[0], inplace = True)
                            # Comprobamos si se han eliminado los valores atípicos
                            display('Filas,columnas sin valores atipicos: ', titanic_copia.shape)
                        </py-repl>
                        Podemos comprobar que se han eliminado correctamente los datos atípicos de la columna 'Age'.

                        <h3>Datos duplicados</h3>
                        Una vez eliminada la información irrelevante para el análisis, comprobaremos la existencia de datos duplicados. En caso de detectar dichos datos duplicados, se procederá a eliminar las filas del conjunto de datos con dichos datos.
                        <br />Para realizar este proceso, usaremos el método duplicated() para comprobar si un registro está duplicado o no, junto con una suma o conteo de los mismos.
                        <br />Para eliminar las filas duplicadas, usaremos la función drop_duplicates.
                        <py-repl>
                            if titanic_copia.duplicated(keep = 'first').sum() > 0:
                                titanic_copia = titanic_copia.drop_duplicates(keep='first')
                            display(titanic_copia)
                        </py-repl>
                        <em>Explicación</em>
                        <ul>
                            <li>Usamos el condicional if para comprobar si se cumple la condición de que el dataset tenga filas duplicadas, es decir, que la suma de las filas duplicadas sea superior a 0.</li>
                            <li>Si se cumple la condición, se eliminan las filas duplicadas del conjunto de datos. En caso contrario, no hace nada.</li>
                            <li>En la función duplicated, keep='first' significa que marca como True las filas duplicadas, excepto la primera ocurrencia.</li>
                            <li>En la función drop_duplicates, keep='first' significa que elimina las filas duplicadas excepto la primera ocurrencia.</li>
                        </ul>
                        Dada la información obtenida mediante el código anterior, podemos comprobar que se han reducido el número de filas del conjunto de datos.
                        <br />Dicha reducción de filas significa que las filas duplicadas del conjunto de datos han sido eliminadas correctamente.
                        <h3>Datos perdidos o faltantes</h3>
                        Cuando cargamos un conjunto de datos usando Pandas, todos los datos perdidos se convierten automáticamente en valores "NaN". <br />
                        La manera más sencilla de detectar los valores pérdidos es usar la función insnull() de Pandas, que devuelve los siguientes 2 valores:
                        <ul>
                            <li>True, cuando se trata de un valor perdido.</li>
                            <li>False, cuando no se trata de un valor perdido.</li>
                        </ul>
                        Además, si utilizamos también la función sum() podemos comprobar cuántos valores perdidos hay en cada columna del conjunto de datos. Veamos cómo funciona en el siguiente código:
                        <py-repl>
                            display(titanic_copia.isnull().sum())
                        </py-repl>
                        Si observamos los resultados obtenidos, podemos comprobar que la columna 'Age' tiene 177 datos perdidos, la columna 'Cabin' tiene 687 datos perdidos, y la columna 'Embarked' tiene 2 datos perdidos.
                        <br />Nota: Otra forma de visualizar los datos perdidos en un gráfico es usando los mapas de calor que proporciona la librería seaborn, tal y como veremos en el siguiente código:
                        <py-repl>
                            import seaborn as sns
                            import matplotlib.pyplot as plt
                            sns.heatmap(titanic_copia.isnull(), cbar=False)
                            display(plt)
                        </py-repl>
                        Las marcas blancas del grafico anterior representan los valores perdidos, de modo que podemos encontrar patrones y vínculos existentes entre los valores perdidos en las diferentes columnas del dataset.
                        <br />
                        Una vez hemos detectado e identificado correctamente los valores perdidos, podemos ocuparnos de ellos mediante su eliminación o imputación.
                        <h4>Eliminación de valores faltantes</h4>
                        Para eliminar los valores faltantes, tan solo tenemos que utilizar la función dropna que nos proporciona Pandas. Sin embargo, esta opción no es recomendable para conjuntos de datos pequeños o con un alto procentaje de valores perdidos.
                        <br />Para este caso, vamos a eliminar la columna Cabin debido a la tasa elevada de valores faltantes que contiene:
                        <py-repl>
                            titanic_copia.drop(['Cabin'], axis=1, inplace=True)
                            # Comprobamos si se ha eliminado dicha columna
                            display(titanic_copia)
                        </py-repl>
                        <h4>Imputación de valores faltantes</h4>
                        Actualmente, existen 2 aproximaciones comúnmente usadas para la imputación de valores perdidos.
                        <br />La primera técnica consiste en rellenar estos valores con la media o la mediana de los datos de la variable en caso de que se trate de una variable numérica.
                        <br />Para el caso de variables categóricas imputamos los valores perdidos con la moda de la variable.
                        <br />De nuevo Pandas nos ofrece funciones para calcular la media (mediante la función mean()), la mediana (mediante la función median()) y la moda (mediante la función mode()).
                        <py-repl>
                            # Aplicamos la mediana para rellenar los valores faltantes de la columna Age
                            titanic_copia['Age'].fillna(titanic_copia['Age'].median(), inplace=True)
                            # Comprobamos si se han relleando los valores faltantes
                            display(titanic_copia.isnull().sum())
                        </py-repl>
                        <h3>Transformación de datos</h3>
                        A continuación, veremos distintos métodos de transformación de datos.
                        <h4>Categorías de datos</h4>
                        Para analizar las categorías de los datos del dataset, Pandas nos proporciona la función info().
                        <py-repl>
                            display(titanic_copia.info())
                        </py-repl>
                        Observamos que este conjunto de datos tiene 3 tipos diferentes de datos:
                        <ul>
                            <li>float64(2), significa que el conjunto de datos tiene 2 columnas de valores flotantes (con decimales).</li>
                            <li>int64(4), significa que el conjunto de datos tiene 4 columnas con valores enteros (sin decimales).</li>
                            <li>object(1), significa que el conjunto de datos tiene 1 columnas con valores objeto</li>
                        </ul>
                        Para calcular y realizar análisis, no podemos usar datos de tipo objeto. Por tanto, debemos convertir los datos de tipo objeto en tipo flotante o entero.
                        <br /> Existen diversos métodos para transformar los datos categóricos en numéricos. Pandas nos proporciona la función replace que permite modificar conjuntos de valores por otros.
                        <py-repl>
                            df = titanic_copia.copy()
                            df['Sex'] = df['Sex'].replace(['female','male'],[0,1])
                            display(df)
                        </py-repl>
                        Sin embargo, un método más cómodo para realizar dicha transformación es usando la función get_dummies() de Pandas.
                        <py-repl>
                            titanic_copia = pd.get_dummies(titanic_copia, columns = ["Sex"], prefix=["Sex"])
                            display(titanic_copia.head())
                        </py-repl>
                        Podemos comprobar que se han añadido 2 nuevas columnas en el conjunto de datos, una para el sexo femenino y otra para el sexo masculino.
                        <h4>Combinación de columnas</h4>
                        Otra técnica de la transformación de datos consiste en combinar la información de las columnas del conjunto de datos. En nuestro caso, tenemos que las columnas SibSp y Parch representan la siguiente información:
                        <ul>
                            <li>SibSp, representa si el pasajero viaja con hermanos o no.</li>
                            <li>Parch, representa si el pasajero viaja con padres o no.</li>
                        </ul>
                        Dicha información podemos unificarla creando una nueva columna que represente el tamaño de la familia del pasajero.
                        <br />Para ello, creamos una nueva columna en nuestro conjunto de datos con dicha información y eliminaremos la información de las columnas SibSp y Parch.
                        <py-repl>
                            titanic_copia['FamilySize'] = titanic_copia['SibSp'] + titanic_copia['Parch'] + 1
                            titanic_copia.drop(['SibSp','Parch'], axis=1, inplace=True)
                            display(titanic_copia.head())
                        </py-repl>
                        <h4>Contenedores de datos</h4>
                        Otra técnica que podemos aplicar es crear contenedores de datos, es decir, si tenemos datos con numeros valores podemos agruparlos en datos más sencillos como por ejemplo, en rangos de valores.
                        <br />En nuestro caso, podemos crear contenedores de datos para las columnas Age y Fare. Para realizar este proceso, Pandas nos aporte la función cut.
                        <py-repl>
                            # Contenedor de datos de la columna Age
                            titanic_copia['Age_bin'] = pd.cut(titanic_copia['Age'], bins=[0,12,20,40,120], labels=['Children','Teenage','Adult','Elder'])
                            # Contenedor de datos de la columna Fare
                            titanic_copia['Fare_bin'] = pd.cut(titanic_copia['Fare'], bins=[0,7.91,14.45,31,120], labels=['Low_fare','median_fare','Average_fare','high_fare'])
                            # Eliminamos las columnas Age y Fare
                            titanic_copia.drop(['Age','Fare'], axis=1, inplace=True)
                            # Mostramos el conjunto de datos
                            display(titanic_copia.head())
                        </py-repl>
                        Si observamos el conjunto de datos, podemos comprobar que se han creado 2 nuevas columnas pero que contienen datos categóricos. Como vimos anteriormente con el Sexo, podemos transformar el tipo de objeto usando la función get_dummies
                        <py-repl>
                            titanic_copia = pd.get_dummies(titanic_copia, columns = ["Age_bin", 'Fare_bin'], prefix=["Age_type", 'Fare_type'])
                            display(titanic_copia.head())
                        </py-repl>
                        <h3>Análisis de los datos</h3>
                        Una vez aplicado el proceso de transformación de datos, podemos comenzar a analizar los datos. Para realizar este proceso, aplicaremos principalmente la libreria seaborn y analizaremos los resultados obtenidos enfocándonos a la meta establecida para el conjunto de datos
                        <br />Nota: Este proceso se puede hacer en este momento o al final, al obtener conclusiones, debido a que se emplearán los métodos que se mostrarán a continuación para las conclusiones
                        Lo primero que debemos hacer es detectar la característica principal del conjunto de datos. En nuestro caso, la característica principal será la información proporcionada por la columna Survived, pues su información indica si el pasajero sobrevivió o no.
                        <h4>Datos balanceados y no desbalanceados</h4>
                        Al analizar un conjunto de datos, es importante comprobar si los datos están balanceados o desbalanceados. En el código a continuación veremos como comprobarlo:
                        <py-repl>
                            import seaborn as sns
                            import matplotlib.pyplot as plt
                            sns.countplot( x='Survived', data=titanic_copia)
                            plt.title('Distribucion de supervivientes/muertos del titanic')
                            display(plt)
                        </py-repl>
                        <em>Explicación del código</em>
                        <ul>
                            <li>Usamos la función countplot de la librería seaborn para generar un gráfico de barras.</li>
                            <li>x='Survived' significa que el eje X del gráfico representará la información correspondiente a la columna 'Survived'.</li>
                            <li>data=titanic_copia significa que se empleará para generar el gráfico el conjunto de datos titanic_copia</li>
                            <li>Usamos la función title de la librería matplotlib para añadir un título al gráfico generado.</li>
                        </ul>
                        Si analizamos el gráfico anterior, podemos comprobar que los datos están desbalanceados, habiendo más pasajeros muertos que pasajeros vivos.
                        <br />En el código a continuación, vamos a analizar aquellos pasajeros con mayor tasa de supervicencia en función del sexo femenino.
                        <py-repl>
                            import seaborn as sns
                            import matplotlib.pyplot as plt
                            # Generamos el grafico usando la funcion catplot de seaborn
                            sns.catplot(x='Sex_female', hue='Survived', kind='count', data=titanic_copia)
                            # Mostramos el grafico generado
                            display(plt)
                            # Limpiamos el grafico generado
                            plt.clf()
                        </py-repl>
                        Si observamos la gráfica, podemos comprobar que sobrevivieron más mujeres que hombres en el hundimiento del titanic. 
                        <br />Otro estudio que podemos realizar es si el tipo de clase del pasajero influyó en la supervivencia de los pasajeros.
                        <py-repl>
                            import seaborn as sns
                            import matplotlib.pyplot as plt
                            # Generamos el grafico usando la funcion catplot de seaborn
                            sns.catplot(x='Pclass', hue='Survived', kind='count', data=titanic_copia)
                            # Mostramos el grafico generado
                            display(plt)
                            # Limpiamos el grafico generado
                            plt.clf()
                        </py-repl>
                        Si observamos el gráfico, podemos comprobar que tuvieron mayor tasa de supervivencia las personas con una clase de tiquet más elevado. 
                        Otros posibles análisis a realizar, sería analizar si influye en la tasa de supervivencia la edad de la persona, si iban en familia o no, etc.
                    </p>

                </div>
                <!--Paso 3: Algoritmos-->
                <div id="Algoritmos" style="">
                    <h1>Paso 3. Algoritmos</h1>
                    En esta sección veremos distintos métodos de implementar algoritmos de minería de datos, así como distintas métricas para analizar el los resultados obtenidos en dichos algoritmos.
                    <br />Para este tutorial, se utilizará el conjunto de datos iris.
                    <h2>Obtención del conjunto de datos</h2>
                    En el código a continuación, obtendremos el conjunto de datos iris
                    <py-repl>
                        import pandas as pd
                        from pyodide.http import open_url
                        iris = pd.read_csv(open_url("https://raw.githubusercontent.com/p72losaj/Datasets/main/iris.data"),
                        header=None,
                        sep=",",
                        names = ["sepal length","sepal width","petal length","petal width","Species"])
                        display(iris.head())
                    </py-repl>
                    <h2>Preprocesamiento de datos</h2>
                    Antes de comenzar a diseñar los distintos algoritmos de minería de datos, es recomendable realizar un análisis y, en caso de ser necesario, un preprocesamiento de los datos del dataset.
                    <br />Nota: Se establecerá como meta final predecir las especies de iris
                    <br />Si visualizamos el conjunto de datos, podemos comprobar que no existe información innecesaria, es decir, no podemos eliminar ninguna columna del conjunto de datos.
                    <br />Este proceso lo veremos en el código a continuación:
                    <py-repl>
                        import seaborn as sns
                        import matplotlib.pyplot as plt
                        display('Comprobando la existencia de valores perdidos en el conjunto de datos iris'); display(iris.isnull().sum())
                        display('Analizando el numero de elementos por clase del conjunto de datos'); display(iris['Species'].value_counts())
                        # Comprobamos la existencia de datos atipicos
                        sns.boxplot(x=iris['Species'],y=iris['sepal length']);plt.title('Datos atipicos sepal length');display(plt);plt.clf()
                        sns.boxplot(x=iris['Species'],y=iris['sepal width']); plt.title('Datos atipicos sepal width'); display(plt);plt.clf()
                        sns.boxplot(x=iris['Species'],y=iris['petal length']);plt.title('Datos atipicos petal length');display(plt);plt.clf()
                        sns.boxplot(x=iris['Species'],y=iris['petal width']); plt.title('Datos atipicos petal width'); display(plt);plt.clf()
                        # Comprobamos la existencia de datos duplicados
                        display('Numero de datos duplicados: ', iris.duplicated(keep='first').sum())
                    </py-repl>
                    A partir del código anterior, observamos la siguiente información:
                    <ul>
                        <li>El conjunto de datos no contiene valores perdidos.</li>
                        <li>El conjunto de datos está balanceado, es decir, las clases de iris tienen el mismo número de elementos.</li>
                        <li>Aunque existen valores atípicos, la cantidad de los mismos es muy reducida. En nuestro tutorial, vamos a optar por dejar dichos datos atípicos.</li>
                        <li>
                            Aunque existen datos duplicados, comprobamos que sólo existen 3 filas duplicadas. Por este motivo, y teniendo en cuenta que si eliminamos dichos datos se puede producir un desbalanceamiento de los datos
                            , para este tutorial no vamos a eliminar dichos datos duplicados.
                        </li>
                        <li>Si visualizamos la información del dataset, podemos comprobar que sólo tiene datos no numéricos el atributo de clase. En este caso, no vamos a transformar los datos categóricos en datos numéricos.</li>
                    </ul>
                    <h2>Modelado de los datos</h2>
                    Una vez realizado el preprocesamiento de los datos, debemos separar la información objetivo (característica de clase) del resto de información. Esto se conoce como modelado de datos.
                    <br />En el código a continuación, aplicaremos la biblioteca scikit-learn para modelar los datos:
                    <py-repl>
                        # Dividimos la información
                        X = iris.drop(['Species'], axis=1); Y = iris['Species']
                        # Comprobamos que se ha dividido correctamente la informacion
                        display(X.head()); display(Y.head())
                    </py-repl>
                    Si ejecutamos el código anterior, podemos comprobar la separación de la información de la etiqueta de clase del resto de información del conjunto de datos.
                    <br />A continuación, podemos entrenar y realizar pruebas de los datos mediante distintos algoritmos de minería de datos. Tenemos 2 formas de hacerlo
                    <ol>
                        <li>Entrenamiento y test del mismo conjunto de datos: Existe riesgo de sobreentrenamiento de los datos.</li>
                        <li>
                            Dividiendo el conjunto de datos en datos de entrenamiento y datos de test: Permite asegurarnos de no usar las mismas observaciones en todos los datos
                            de entrenamiento.
                            <br /> Inconveniente: Las puntuaciones de precisión del conjunto de pruebas pueden variar según las observaciones que haya en el conjunto. Sin embargo, podemos solucionar este problema usando validación cruzada.
                        </li>
                    </ol>
                    Nota: Al diseñar los algoritmos de mineria de datos, es recomendable realizar una búsqueda de los mejores parámetros de diseño del algoritmo. Para ello, un método eficaz es usando GridSearchCV.
                    <br />Para este tutorial, diseñaremos los algoritmos aplicando los méjores parámetros.
                    <h2>Árbol de Decisión</h2>
                    En esta sección, utilizaremos el conjunto de datos iris para diseñar el siguiente algoritmo de minería de datos: Árbol de decisión.
                    <br />Además, estudiaremos distintas técnicas a aplicar tanto al conjunto de datos y al algoritmo, con el objetivo de obtener los mejores resultados posibles.
                    <br />Un árbol de decisión es un algoritmo de aprendizaje supervisado no paramétrico, que se utiliza tanto para tareas de clasificación como de regresión.
                    <br />Para el diseño de los árboles de decisión, podemos usar 2 criterios:
                    <ul>
                        <li>Índice gini: El índice Gini es la probabilidad de que una variable no clasifique correctamente si se eligió al azar.</li>
                        <li>Entropía: La entropía es la medida de la impureza de un conjunto de datos; alternativamente, podemos pensar en esto como la medida de incertidumbre del grupo.</li>
                    </ul>

                    <h3>Mismo conjunto de datos</h3>
                    En esta sección, diseñaremos distintos árboles de decisión aplicando el dataset iris sin división del mismo en datos de entrenamiento y de test.
                    <br />En el código a continuación, diseñaremos un árbol de decisión sobre los datos originales del dataset iris:
                    <py-repl>
                        # Importamos las librerias necesarias
                        from sklearn import metrics
                        from sklearn.tree import DecisionTreeClassifier
                        from sklearn.model_selection import GridSearchCV
                        # Creamos el arbol de decision
                        decision_tree = DecisionTreeClassifier()
                        # Establecemos los parametros para gridsearchcv
                        param_dict = {
                            "criterion": ["gini", "entropy"],
                            "max_depth": range(1,10),
                            "min_samples_split": range(1,10),
                            "min_samples_leaf": range(1,5)
                        }
                        # Encontramos el mejor hiperparámetro usando gridsearchCV
                        grid = GridSearchCV(decision_tree,
                                            param_grid = param_dict,
                                            cv=5,
                                            verbose=1,
                                            n_jobs=-1
                        )
                        # Entrenamos el modelo
                        grid.fit(X,Y)
                    </py-repl>
                    En el código a continuación, obtendremos los mejores resultados obtenidos:
                    <py-repl>
                        display('Mejores parámetros: ', grid.best_params_)
                        display('Mejor estimador: ', grid.best_estimator_)
                        display('Mejor resultado: ', grid.best_score_)
                    </py-repl>
                    Si ejecutamos el código anterior, podemos visualizar los hiperparámetros del mejor árbol de decisión posible para el conjunto de datos empleado.
                    <br />Además, podemos comprobar un resultado final obtenido del 90%, con lo cuál tenemos un algoritmo muy bueno.
                    <br />Nota: Al diseñar algoritmos usando GridSearchCV, es recomendable obtener un score superior al 80%.
                    <br />Otro tipo de árbol de decisión que podemos generar, es un árbol de decisión aleatorio. En el código a continuación, vamos a generar un arbol aleatorio:
                    <br />Aviso: Debido a la búsqueda de hiperparámetros, puede producirse un fallo de página al emplear la búsqueda GridSearchCV.
                    <py-repl>
                        # Importamos las librerias necesarias
                        from sklearn import metrics
                        from sklearn.ensemble import RandomForestClassifier
                        from sklearn.model_selection import GridSearchCV
                        # Establecemos los parametros para gridsearchcv
                        param_dict = {
                                        "criterion": ["gini","entropy"],
                                        "max_depth": range(1,3),
                                        "min_samples_split": range(2,5),
                                        "min_samples_leaf": range(1,3)
                        }
                        # Encontramos el mejor hiperparámetro usando gridsearchCV
                        grid = GridSearchCV(RandomForestClassifier(),
                                            param_grid = param_dict,
                                            cv=5,
                                            verbose=1,
                                            n_jobs=-1
                                            )
                        # Entrenamos el modelo
                        grid.fit(X,Y)
                    </py-repl>
                    En el código a continuación, visualizamos los mejores resultados obtenidos:
                    <py-repl>
                        display('Mejores parámetros: ', grid.best_params_)
                        display('Mejor estimador: ', grid.best_estimator_)
                        display('Mejor resultado: ', grid.best_score_)
                    </py-repl>
                    Nota: Tener en cuenta que al ser un árbol aleatorio el mejor resultado puede cambiar. Por ello, es recomendable ejecutar el código una cierta cantidad de veces.
                </div>
                
                <!--Paso 4: Conclusiones-->

                <!--Ejercicios propuestos-->
                <div id="EjerciciosPropuestos" style="display:none">
                    <h1>Ejercicios propuestos</h1>
                    <p>
                        A continuación, se popondrán un conjunto de ejercicios a resolver por el usuario.
                        <h3>Ejercicio 1. Aplica las etapas del proceso de minería de datos al siguiente conjunto de datos</h3>
                        <py-repl>
                            url_dataset = https://raw.githubusercontent.com/p72losaj/Datasets/main/wine.data
                            col_names = ['class', 'Alcohol', 'Malic_acid', 'Ash', 'Alcalinity_of_ash', 'Magnesium', 'Total_phenols','Flavanoids', 'Nonflavanoid_phenols', 'Proanthocyanins', 'Color_intensity', 'Hue', 'OD280/OD315','Proline']
                        </py-repl>
                        <h3>Ejercicio 2. Aplica las etapas del proceso de minería de datos al siguiente conjunto de datos</h3>
                        <py-repl>
                            url_dataset = https://raw.githubusercontent.com/p72losaj/Datasets/main/diabetes.csv
                            col_names = ['preg', 'plas','pres','skin','insu','mass','pedi','age','class']
                        </py-repl>
                        <h3>Ejercicio 3. Aplica las etapas del proceso de minería de datos al siguiente conjunto de datos</h3>
                        <py-repl>
                            url_dataset = https://raw.githubusercontent.com/p72losaj/Datasets/main/abalone.data
                            col_names = ['Sex', 'Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight', 'Rings/Class']
                        </py-repl>
                        <h3>Ejercicio 4. Aplica las etapas del proceso de minería de datos al siguiente conjunto de datos</h3>
                        <py-repl>
                            url_dataset = https://raw.githubusercontent.com/p72losaj/Datasets/main/glass.arff
                        </py-repl>
                        <h3>Ejercicio 5. Aplica las etapas del proceso de minería de datos al siguiente conjunto de datos</h3>
                        <py-repl>
                            url_dataset = https://raw.githubusercontent.com/p72losaj/Datasets/main/Dry_Bean_Dataset.arff
                        </py-repl>
                        <h3>Ejercicio 6. Aplica las etapas del proceso de minería de datos al siguiente conjunto de datos</h3>
                        <py-repl>
                            url_dataset = https://raw.githubusercontent.com/p72losaj/Datasets/main/car.data
                            col_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']
                        </py-repl>
                        <h3>Ejercicio 7. Aplica las etapas del proceso de minería de datos al siguiente conjunto de datos</h3>
                        <py-repl>
                            url_dataset = https://raw.githubusercontent.com/p72losaj/Datasets/main/zoo.data
                            col_names = ['animal_name', 'hair', 'feathers', 'eggs', 'milk', 'airzorne', 'aquatic', 'predator', 'toothed', 'backbone', 'breathes', 'venomous', 'fins', 'legs', 'tail', 'domestic', 'catsize', 'class']
                        </py-repl>
                    </p>
                </div>
            </div>
        </div>
    </div>

    
    <!--Py-script-->
    <div class="pyscript">

        <!-- Codigo python-->
        <py-script>
            
            from sklearn.model_selection import cross_val_score
            from sklearn.tree import DecisionTreeClassifier
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.neighbors import KNeighborsClassifier
            from sklearn.svm import SVC
            from sklearn.metrics import confusion_matrix


            def Classifiers(train_data, test_data):
            """ Apply classifier of mining data to dataset
            :param train_data: Train Data of Dataset
            :param test_data: Test data of Dataset
            """
            # Create models
            models = [DecisionTreeClassifier(), KNeighborsClassifier(n_neighbors=3), RandomForestClassifier(), SVC(probability = True)]
            classifiers = ['DecisionTree', 'KNN', 'RandomForest', 'SVC']
            log_cols = ["Classifier", "Accuracy"]
            log = pd.DataFrame(columns=log_cols)
            acc_dict = {}
            # train and test
            i=0
            for model in models:
            name = model.__class__.__name__
            acc = cross_val_score(model, train_data, test_data, cv=5, scoring='accuracy').mean()
            log.loc[i, log_cols[0]] = models[i]
            log.loc[i, log_cols[1]] = acc
            i += 1
            # Show result
            display(log)
            plt.xlabel('Classifiers')
            plt.title('Classifier Accuracy')
            sns.barplot(x=classifiers, y=log.Accuracy, data=log, color='b')
            display(plt)
            plt.clf()

            

            def showTitanicFarePclassSurvived(df):
            """ Analysis of the survivors according to the fare and pclass of titanic dataset
            :param df: Dataset titanic
            """
            sns.catplot(x='Fare', hue='Survived', kind='count', col='Pclass', data=df)
            display(plt)
            plt.clf()

            def showTitanicMembersFamilySurvived(df):
            """ Analysis of the survivors according to the member family of titanic dataset
            :param df: Dataset titanic
            """
            sns.catplot(x='Members_Family', hue='Survived', kind='count', data=df)
            display(plt)
            plt.clf()

            def generate_confusion_matrix(train,test,classifier):
            """ Generate a confusion matrix
            :param train: Data train
            :param test: Data test
            :classifier: Mining Data Classifier
            """
            classifier.fit(train,test)
            display(test.unique())
            cm = confusion_matrix(test, classifier.predict(train))
            display(cm)

        </py-script>
        
        
        <!--Ejercicio 3-->
        <p>
            <em>Ejercicio 3: Aplica los siguientes algoritmos a cada dataset: Arbol de Decision, Arbol de decision aleatorio, KNN, SVC </em>
        </p>
        <py-repl>
            # Datos de entrenamiento del dataset titanic
            train_titanic = titanic2.drop(['Survived'], axis=1)
            # Datos de test del dataset titanic
            test_titanic = titanic2['Survived']
            # Aplicamos algoritmos de mineria de datos para el dataset titanic
            Classifiers(train_titanic, test_titanic)
        </py-repl>
        <!--Ejercicio 4-->
        <p>
            <em>Ejercicio 4: Utiliza la informacion obtenida en los ejercicios anteriores y saca conclusiones </em>
        </p>
        <py-repl>
            # Analysis dataset titanic
            generate_confusion_matrix(train_titanic, test_titanic, SVC(probability=True))
            showTitanicSurvived(titanic2)
            showTitanicAgeSexSurvived(titanic2)
            showTitanicFarePclassSurvived(titanic2)
            showTitanicMembersFamilySurvived(titanic2)
        </py-repl>

    </div>



</body>
</html>

