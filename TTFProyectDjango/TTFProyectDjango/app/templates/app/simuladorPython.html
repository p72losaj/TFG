<!DOCTYPE html>

<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>{% block title %}NETTLAERER{% endblock %}</title>

    <!--Archivos navbar-->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css"
          integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

    <!--Archivos pyscript-->
    <link rel="stylesheet" href="https://pyscript.net/latest/pyscript.css" />
    <script defer src="https://pyscript.net/latest/pyscript.js"></script>

    <!-- Add CSS style-->
    <link href="//fonts.googleapis.com/css?family=Lobster&subset=latin,latin-ext" rel="stylesheet" type="text/css">

    <!-- Configuracion PyScript -->
    <py-config>
        packages = [
        "pandas",
        "seaborn",
        "matplotlib",
        "scipy",
        "scikit-learn",
        'opencv-python',
        ]
        terminal = false
        docked = false
    </py-config>

    <style>

        .header {
            padding-left: 50px;
            margin-top: 50px;
        }

        .menuBotones {
            text-align: center;
            padding: 50px;
        }

        .informacion {
            padding: 50px;
            text-align: left
        }

        .pyscript {
            padding-left: 50px;
            margin-top: 50px;
        }

        p {
            font-family: 'Lobster';
            text-align: left
        }

        table {
            margin-top: 30px;
            border-collapse: collapse;
        }
    </style>
</head>
<body>
    <!--Add a navbar header-->
    <div class="header">
        <h1><em>Menu Principal</em></h1>
        <nav class="navbar navbar-expand-lg navbar-light ">

            <ul class="nav">
                <li><a href="/" class="navbar-brand">NETTLAERER</a></li>
                <li>
                    <a href="{% url 'tecnologias' %}" class="navbar-brand">TECNOLOGIAS</a>
                </li>
                <li><a href="{% url 'MineriaDatos' %}" class="navbar-brand">Mineria de Datos</a></li>
            </ul>
        </nav>
    </div>
    
    <!--Contenedor de fluido-->
    <div class="container-fluid">
        <div class="row">
            <!--Añadimos un menu de filtro-->
            <div class="col-sm-10">
                <em>Etapas del proceso de mineria de datos</em>
                <br />
                <button onclick="$('#ObtencionDatos').show(); $('#PreprocesamientoDatos').hide(), $('#EjerciciosPropuestos').hide(), $('#Algoritmos').hide(), $('#conclusiones').hide()">Paso 1. Obtención de Datos</button>
                <button onclick="$('#PreprocesamientoDatos').show();$('#ObtencionDatos').hide(), $('#EjerciciosPropuestos').hide(), $('#Algoritmos').hide(), $('#conclusiones').hide()">Paso 2. Preprocesamiento de Datos</button>
                <button onclick="$('#Algoritmos').show(),$('#PreprocesamientoDatos').hide();$('#ObtencionDatos').hide(), $('#EjerciciosPropuestos').hide(), $('#conclusiones').hide()">Paso 3. Algoritmos de minería de datos</button>
                <button onclick="$('#conclusiones').show(),$('#EjerciciosPropuestos').hide(), $('#PreprocesamientoDatos').hide(), $('#ObtencionDatos').hide(),$('#Algoritmos').hide()">Paso 4. Conclusiones</button>
                <button onclick="$('#EjerciciosPropuestos').show(), $('#PreprocesamientoDatos').hide(), $('#ObtencionDatos').hide(),$('#Algoritmos').hide(), $('#conclusiones').hide()">Ejercicios propuestos</button>

            </div>
            
            <div class="col-sm-10">
                <!--Paso 1: Obtencion de datos-->
                <div id="ObtencionDatos" style="display:none">
                    <!--PASO 1: OBTENCION DE DATOS-->
                    <h1>Paso 1: Extracción y lectura de datos</h1>
                    <p>
                        Antes de que podamos analizar los datos, dichos datos deben ser importados o extraídos. <br />
                        A continuación, veremos distintos ejemplos sobre cómo cargar e importar datos a partir de archivos, en formato csv, arff y data, mediante un enlace web.
                        <h2>Archivos CSV</h2>
                        En los ejemplos a continuación, veremos cómo obtener información a partir de archivos en formato csv.
                        <br />
                        Nota: Para obtener los conjuntos de datos, debemos usar la librería Pandas y pyodide.http; sin embargo, antes de obtener dichos datos debemos comprobar si el conjunto de datos se encuentra etiquetado o no etiquetado.<br />
                        <ul>
                            <li>Conjunto de datos etiquetado: Conjunto de datos que incluye las etiquetas de las columnas.</li>
                            <li>Conjunto de datos no etiqueta: Conjunto de datos que no incluye las etiquetas de las columnas.</li>
                        </ul>
                        <h3>Conjunto de datos etiquetado</h3>
                        En esta sección, obtendremos el conjunto de datos titanic. Dicho conjunto de datos contiene información sobre los pasajeros del titanic quienes fueron víctimas
                        de los naufragios más infames de la historia.
                        <br />Dicho conjunto de datos aporta la siguiente información:
                        <ul>
                            <li>PassengerId: Representa el identificador del pasajero</li>
                            <li>Survived: Representa la supervivencia del pasajero (0=No, 1=Sí).</li>
                            <li>Pclass: Representa la clase del ticket (1=Primero, 2=Segundo, 3=Tercero).</li>
                            <li>Name: Representa el nombre del pasajero</li>
                            <li>Sex: Representa el sexo del pasajero (male=Masculino, female=Femenino)</li>
                            <li>Age: Representa la edad del pasajero, en años</li>
                            <li>SibSp: Representa el número de hermanos o cónyuges a bordo del Titanic</li>
                            <li>Parch: Representa el número de padres o hijos a bordo del Titanic</li>
                            <li>Ticket: Representa el número del ticket</li>
                            <li>Fare: Representa la tarifa del pasajero</li>
                            <li>Cabin: Representa el número de la cabina</li>
                            <li>Embarked: Representa el puerto de embarque</li>
                        </ul>
                        En el código a continuación, veremos como obtener el conjunto de datos titanic a partir del enlace web:
                        <py-repl>
                            import pandas as pd
                            from pyodide.http import open_url
                            titanic = pd.read_csv(open_url("https://raw.githubusercontent.com/p72losaj/Datasets/main/titanic.csv"),header=0, sep=",")
                            display(titanic)
                        </py-repl>
                        <em>Explicación del ejemplo</em>
                        <ul>
                            <li>Importamos las librerías pandas y pyodide.http</li>
                            <li>Nombramos el dataFrame como titanic</li>
                            <li>header=0 significa que las cabeceras de los nombres de las variables se encuentran en la primera fila.</li>
                            <li>sep="," significa que "," es usado como el separador entre las variables</li>
                        </ul>
                        Consejo: Si obtenemos un conjunto de datos con un número elevado de filas, podemos usar la función head(x) para mostrar las primeras x filas del conjunto de datos.
                        <br />Si no especificamos un valor de x, por defecto se muestran las primeras 5 filas del conjunto de datos.
                        <br />En el código a continuación, veremos como mostrar las primeras 5 filas de un conjunto de datos:
                        <py-repl>
                            import pandas as pd
                            from pyodide.http import open_url
                            titanic = pd.read_csv(open_url("https://raw.githubusercontent.com/p72losaj/Datasets/main/titanic.csv"),
                            header=0,
                            sep=",")
                            display(titanic.head())
                        </py-repl>
                        <h3>Conjunto de datos no etiquetado</h3>
                        Para esta sección, se obtendrá el conjunto de datos iris. El conjunto de datos iris contiene información de un estudio de 3 especies de Iris (Iris setosa, Iris Virginica e Iris versicolor).
                        <br />Dicho conjunto de datos aporta la siguiente información:
                        <ul>
                            <li>sepal length: Representa la longitud del sépalo, en centímetros.</li>
                            <li>sepal width: Representa la anchura del sépalo, en centímetros.</li>
                            <li>petal length: Representa la longitud del pétalo, en centímetros.</li>
                            <li>petal width: Representa la anchura del pétalo, en centímetros.</li>
                            <li>Species: Representa las especies de Iris</li>
                        </ul>
                        <br />En el código a continuación, veremos un método para obtener dicho conjunto de datos.
                        <py-repl>
                            import pandas as pd
                            from pyodide.http import open_url
                            iris = pd.read_csv(open_url("https://raw.githubusercontent.com/p72losaj/Datasets/main/iris.data"),
                            header=None,
                            sep=",",
                            names = ["sepal length","sepal width","petal length","petal width","Species"])
                            display(iris.head(7))
                        </py-repl>
                        <em>Explicación del ejemplo</em>
                        <ul>
                            <li>Importamos las librerías pandas y pyodide.http</li>
                            <li>Nombramos el dataFrame como titanic</li>
                            <li>header=1 significa que las cabeceras de los nombres de las variables no se encuentran en la primera fila.</li>
                            <li>sep="," significa que "," es usado como el separador entre las variables</li>
                            <li>names = ["sepal length","sepal width","petal length","petal width","Species"] significa la secuencia de etiquetado de las columnas del conjunto de datos a aplicar.</li>
                        </ul>
                    </p>

                    <h2>Archivos ARFF</h2>
                    <p>
                        En esta sección, obtendremos el conjunto de datos segment_challenge. Al ser un archivo arff, toda la información referente al mismo se visualiza en el propio documento.
                        <br />En el código a continuación, veremos como importar los datos de dicho conjunto de datos.
                        <py-repl>
                            import pandas as pd
                            from pyodide.http import open_url
                            from scipy.io import arff
                            segment_challenge = pd.DataFrame(arff.loadarff(open_url("https://raw.githubusercontent.com/p72losaj/Datasets/main/segment-challenge.arff"))[0])
                            display(segment_challenge.head(8))
                        </py-repl>
                        <em>Explicación del ejemplo</em>
                        <ul>
                            <li>Importamos las librerías pandas, pyodide y scipy.io</li>
                            <li>Nombramos el dataframe como segment_challenge</li>
                            <li>Usamos la funcion arff.loadarff para cargar el archivo arff.</li>
                            <li>Usamos la función DataFrame para transformar el archivo arff en un dataframe de pandas</li>
                        </ul>
                        Si visualizamos la información obtenida y la comparamos con la sección data del archivo, podemos observar que se ha producido un error de traducción en la columna class del conjunto de datos
                        <br />Para corregir dicho error de codificación, emplearemos la función decode, tal y como vemos en el código a continuación:
                        <py-repl>
                            segment_challenge['class'] = segment_challenge['class'].str.decode('utf-8')
                            display(segment_challenge['class'].head(10))
                        </py-repl>
                    </p>

                    <h2>Archivos Data</h2>
                    <p>
                        En esta sección, obtendremos el conjunto de datos balance scale. Este conjunto de datos se generó para modelar resultados de experimentos psicológicos.
                        <br />La información que aporta dicho conjunto de datos es la siguiente:
                        <ul>
                            <li>class: Representa el atributo de clase del conjunto de datos</li>
                            <li>left_weight: Representa el peso a la izquierda de la balanza.</li>
                            <li>left_distance: Representa la distancia a la izquierda de la balanza.</li>
                            <li>right_weight: Representa el peso a la derecha de la balanza.</li>
                            <li>right_distance: Representa la distancia a la derecha de la balanza.</li>
                        </ul>
                        En el código a continuación, veremos como obtener la información del conjunto de datos:
                        <py-repl>
                            import pandas as pd
                            from pyodide.http import open_url
                            balance_scale = pd.read_csv(open_url("https://raw.githubusercontent.com/p72losaj/Datasets/main/balance-scale.data"),
                                header=None,
                                sep=",",
                                names = ['class', 'left_weight', 'left_distance', 'right_weight', 'right_distance'])
                            display(balance_scale.head())
                        </py-repl>
                    </p>

</div>
                <!--Paso 2: Preprocesamiento de datos-->
                <div id="PreprocesamientoDatos" style="display:none">
                    <h1>Paso 2. Preprocesamiento de Datos</h1>
                    <p>
                        Una vez importados/extraídos los datos, es importante preprocesar los datos como un paso previo al análisis de los mismos. Para realizar el preprocesamiento de los datos, es importante establecer la meta final de nuestro análisis de los datos. 
                        <br />
                        Recomendación: Antes de comenzar el preprocesamiento de datos, es recomendable establecer la meta final de nuestro análisis debido a que todo este proceso dependerá de dicha meta. <br />
                        Esta meta no es única, es decir, dependerá del conjunto de datos y de la decisión de cada persona.
                        <br />
                    En esta sección, veremos los siguientes pasos para realizar el preprocesamiento de los datos:
                    <ol>
                        <li>Evaluación de los datos: Llevar a cabo una evaluación de la calidad de los datos ayuda a establecer la solidez de los mismos.
                        <li>Limpieza de los datos: La limpieza de los datos tiene como objetivo crear conjuntos de datos sencillos y completos para que los programas ejecuten el análisis.</li>
                        <li>Integrar y transformar los datos: Se utiliza la transformación para convertir los datos en formatos adecuados que el software informático y el aprendizaje automático puedan leer e interpretar.</li>
                    </ol>
                    <p>
                        Al evaluar los datos, es recomendable la realización de una búsqueda de los siguientes datos:
                        <ul>
                            <li>
                                Datos ruidosos, se refiere a aquellos datos sin valor significativo, como las entradas duplicadas o los campos de datos no relevantes para su análisis.
                                <br />La eliminación de los datos innecesarios ayuda a analizar los datos así como disminuir el tiempo de entrenamiento y test de los algoritmos que vayamos a desarrollar, como por ejemplo, árbol de decisión o Regresión Logística.
                            </li>
                            <li>
                                Datos perdidos o faltantes, se refiere a toda pérdida de información por diversas causas como por ejemplo un error humano, un mal funcionamiento u otros factores.
                                <br />La sustitución de estos datos ayuda a garantizar que los futuros análisis sean precisos y fiables.
                            </li>
                            <li>
                                Datos atípicos, se refiere a aquellos datos extraños en el conjunto de datos.
                            </li>
                            <li>
                                Categorías de datos, se refiere a los tipos de datos que componen el conjunto de datos. Los datos pueden ser divididos en 3 categorías: Numéricos, Categóricos u Ordinales.
                            </li>
                        </ul>
                        <h2>El conjunto de datos</h2>
                        Para este tutorial vamos a trabajar con el conjunto de datos titanic. Primero, vamos a obtener el conjunto de datos. Una vez obtenido el conjunto de datos, la meta que vamos a establecer es comprobar quienes fueron los pasajeros del titanic con mayores posibilidades de supervivencia.
                        <py-repl>
                            import pandas as pd
                            from pyodide.http import open_url
                            titanic = pd.read_csv(open_url("https://raw.githubusercontent.com/p72losaj/Datasets/main/titanic.csv"),header=0, sep=",")
                            display(titanic)
                        </py-repl>

                        <h2>Limpieza de Datos</h2>
                        A continuación, analizaremos el dataset titanic y aplicaremos distintas técnicas de detección y limpieza de datos.
                        <h3>Información innecesaria</h3>
                        Si observamos el dataset titanic comprobamos que para la meta establecida la información referida tanto al PassengerId, al ticket del pasajero (Ticket) y al puerto de embarque (Embarked) no aporta información relevante, de modo que podemos eliminar dicha información.
                        <br />Además, consideraremos que la columna Name no aporta la suficiente información relevante para nuestra meta.
                        <br /> Para eliminar las columnas del dataset, emplearemos la función drop de Pamdas, tal y como veremos en el código a continuación:
                        <br />Nota: Es recomendable que al modificar un dataset, realicemos una copia del mismo y modifiquemos dicha copia.
                        <py-repl>
                            titanic_copia = titanic.copy()
                            titanic_copia.drop(['PassengerId','Name','Ticket','Embarked'], axis=1, inplace=True)
                            display(titanic_copia.head())
                        </py-repl>
                        <em>Explicación:</em>
                        <ul>
                            <li>Empleamos la función copy() de Pandas para realizar una copia completa del dataset titanic y almacenamos dicha copia en la variable titanic_copia</li>
                            <li>Para eliminar la columna Ticket, empleamos la función drop de Pandas.</li>
                            <li>axis=1 significa que se elimina la columna completa</li>
                            <li>inplace=True significa que se aplican los cambios directamente sobre el conjunto de datos titanic_copia</li>
                        </ul>
                        <h3>Datos atípicos</h3>
                        Existen diversas formas de detectar datos atípicos, los cuáles abarcan desde generar gráficas hasta el uso de métodos matemáticos.
                        <h4>Diagrama de cajas</h4>
                        <br />Una forma simple y efectiva de visualizar datos atípicos, y resulta prácticamente útil para buscar valores atípicos, es generar diagramas de caja. Para generar estos diagramas, emplearemos la función boxplot de la librería seaborn.
                        <br />Sin embargo, cabe destacar que es recomendable para garantizar una correcta detección de dichos datos, generar dichos gráficos para columnas individuales del conjunto de datos siempre y cuando el conjunto de datos disponga de pocas columnas.
                        <br />En el código a continuación, veremos como generar los diagramas de caja:
                        <py-repl>
                            import seaborn as sns
                            import matplotlib.pyplot as plt
                            sns.boxplot(titanic_copia['Age'])
                            display(plt)
                            # Limpiamos la gráfica generada usando la función clf de matplotlib.pyplot
                            plt.clf()
                        </py-repl>
                        En el gráfico generado, podemos observar que los valores que se encuentren fuera del diagrama se consideran valores atípicos. Al final veremos como eliminar los datos atípicos del conjunto de datos.
                        Es necesario indicar que los diagramas de caja se pueden usar cuando tenemos datos en una sola dimensión.
                        <h4>Rango intercuartílico</h4>
                        Un método usado con mucha frecuencia en la investigación para limpiar datos mediante la eliminación de valores atípicos es calculando el rango intercuartílico para detectar dichos valores.
                        <br />El rango intercuartílico (IQR) es la diferencia entre el tercer cuantil (Q3) y el primer cuantil (Q1). Para calcular dichos valores, el método más sencillo es usando la función describe() de Pandas
                        <py-repl>
                            display(titanic_copia.describe())
                        </py-repl>
                        Si observamos la tabla obtenida, podemos obtener la siguiente información para cada columna del conjunto de datos:
                        <ul>
                            <li>count, significa el número de elementos de la columna.</li>
                            <li>mean, representa el valor medio obtenido a partir de los datos de la columna.</li>
                            <li>std, representa el valor de la desviación típica obtenido a partir de los datos de la columna.</li>
                            <li>min, representa el valor mínimo de la columna.</li>
                            <li>25%, representa el valor del primer percentil (Q1).</li>
                            <li>50%, representa el valor de la mediana obtenido a partir de los datos de la columna.</li>
                            <li>75%, representa el valor del tercer percentil (Q3).</li>
                            <li>max, representa el valor máximo de la columna.</li>
                        </ul>
                        Con los datos de los percentiles Q1 y Q3, podemos definir nuestros límites inferior y superior. Es decir, cualquier punto por debajo del límite inferior y por encima del
                        límite superior se considerará un valor atípico. En nuestro caso, emplearemos este método.
                        <br />En el código a continuación mostraremos cómo detectar y eliminar valores atípicos usando el rango intercuartílico, para la columna Age. Para ello, usaremos la librería numpy para obtener los índices de los valores atípicos:
                        <py-repl>
                            import numpy as np
                            # Mostramos el numero de filas y columnas del conjunto de datos con valores atípicos
                            display('Filas,columnas con valores atipicos: ', titanic_copia.shape)
                            # Calculamos el valor de IQR
                            Q1 = 20.125000; Q3 = 38.000000; IQR = Q3 - Q1
                            # Definimos el límite superior e inferior
                            upper = np.where(titanic_copia['Age'] >= (Q3+1.5*IQR))
                            lower = np.where(titanic_copia['Age'] <= (Q1-1.5*IQR))
                            # Removemos los valores atípicos
                            titanic_copia.drop(upper[0], inplace = True)
                            titanic_copia.drop(lower[0], inplace = True)
                            # Comprobamos si se han eliminado los valores atípicos
                            display('Filas,columnas sin valores atipicos: ', titanic_copia.shape)
                        </py-repl>
                        Podemos comprobar que se han eliminado correctamente los datos atípicos de la columna 'Age'.

                        <h3>Datos duplicados</h3>
                        Una vez eliminada la información irrelevante para el análisis, comprobaremos la existencia de datos duplicados. En caso de detectar dichos datos duplicados, se procederá a eliminar las filas del conjunto de datos con dichos datos.
                        <br />Para realizar este proceso, usaremos el método duplicated() para comprobar si un registro está duplicado o no, junto con una suma o conteo de los mismos.
                        <br />Para eliminar las filas duplicadas, usaremos la función drop_duplicates.
                        <py-repl>
                            if titanic_copia.duplicated(keep = 'first').sum() > 0:
                                titanic_copia = titanic_copia.drop_duplicates(keep='first')
                            display(titanic_copia)
                        </py-repl>
                        <em>Explicación</em>
                        <ul>
                            <li>Usamos el condicional if para comprobar si se cumple la condición de que el dataset tenga filas duplicadas, es decir, que la suma de las filas duplicadas sea superior a 0.</li>
                            <li>Si se cumple la condición, se eliminan las filas duplicadas del conjunto de datos. En caso contrario, no hace nada.</li>
                            <li>En la función duplicated, keep='first' significa que marca como True las filas duplicadas, excepto la primera ocurrencia.</li>
                            <li>En la función drop_duplicates, keep='first' significa que elimina las filas duplicadas excepto la primera ocurrencia.</li>
                        </ul>
                        Dada la información obtenida mediante el código anterior, podemos comprobar que se han reducido el número de filas del conjunto de datos.
                        <br />Dicha reducción de filas significa que las filas duplicadas del conjunto de datos han sido eliminadas correctamente.
                        <h3>Datos perdidos o faltantes</h3>
                        Cuando cargamos un conjunto de datos usando Pandas, todos los datos perdidos se convierten automáticamente en valores "NaN". <br />
                        La manera más sencilla de detectar los valores pérdidos es usar la función insnull() de Pandas, que devuelve los siguientes 2 valores:
                        <ul>
                            <li>True, cuando se trata de un valor perdido.</li>
                            <li>False, cuando no se trata de un valor perdido.</li>
                        </ul>
                        Además, si utilizamos también la función sum() podemos comprobar cuántos valores perdidos hay en cada columna del conjunto de datos. Veamos cómo funciona en el siguiente código:
                        <py-repl>
                            display(titanic_copia.isnull().sum())
                        </py-repl>
                        Si observamos los resultados obtenidos, podemos comprobar que la columna 'Age' tiene 177 datos perdidos, la columna 'Cabin' tiene 687 datos perdidos, y la columna 'Embarked' tiene 2 datos perdidos.
                        <br />Nota: Otra forma de visualizar los datos perdidos en un gráfico es usando los mapas de calor que proporciona la librería seaborn, tal y como veremos en el siguiente código:
                        <py-repl>
                            import seaborn as sns
                            import matplotlib.pyplot as plt
                            sns.heatmap(titanic_copia.isnull(), cbar=False)
                            display(plt)
                        </py-repl>
                        Las marcas blancas del grafico anterior representan los valores perdidos, de modo que podemos encontrar patrones y vínculos existentes entre los valores perdidos en las diferentes columnas del dataset.
                        <br />
                        Una vez hemos detectado e identificado correctamente los valores perdidos, podemos ocuparnos de ellos mediante su eliminación o imputación.
                        <h4>Eliminación de valores faltantes</h4>
                        Para eliminar los valores faltantes, tan solo tenemos que utilizar la función dropna que nos proporciona Pandas. Sin embargo, esta opción no es recomendable para conjuntos de datos pequeños o con un alto procentaje de valores perdidos.
                        <br />Para este caso, vamos a eliminar la columna Cabin debido a la tasa elevada de valores faltantes que contiene:
                        <py-repl>
                            titanic_copia.drop(['Cabin'], axis=1, inplace=True)
                            # Comprobamos si se ha eliminado dicha columna
                            display(titanic_copia)
                        </py-repl>
                        <h4>Imputación de valores faltantes</h4>
                        Actualmente, existen 2 aproximaciones comúnmente usadas para la imputación de valores perdidos.
                        <br />La primera técnica consiste en rellenar estos valores con la media o la mediana de los datos de la variable en caso de que se trate de una variable numérica.
                        <br />Para el caso de variables categóricas imputamos los valores perdidos con la moda de la variable.
                        <br />De nuevo Pandas nos ofrece funciones para calcular la media (mediante la función mean()), la mediana (mediante la función median()) y la moda (mediante la función mode()).
                        <py-repl>
                            # Aplicamos la mediana para rellenar los valores faltantes de la columna Age
                            titanic_copia['Age'].fillna(titanic_copia['Age'].median(), inplace=True)
                            # Comprobamos si se han relleando los valores faltantes
                            display(titanic_copia.isnull().sum())
                        </py-repl>
                        <h3>Transformación de datos</h3>
                        A continuación, veremos distintos métodos de transformación de datos.
                        <h4>Categorías de datos</h4>
                        Para analizar las categorías de los datos del dataset, Pandas nos proporciona la función info().
                        <py-repl>
                            display(titanic_copia.info())
                        </py-repl>
                        Observamos que este conjunto de datos tiene 3 tipos diferentes de datos:
                        <ul>
                            <li>float64(2), significa que el conjunto de datos tiene 2 columnas de valores flotantes (con decimales).</li>
                            <li>int64(4), significa que el conjunto de datos tiene 4 columnas con valores enteros (sin decimales).</li>
                            <li>object(1), significa que el conjunto de datos tiene 1 columnas con valores objeto</li>
                        </ul>
                        Para calcular y realizar análisis, no podemos usar datos de tipo objeto. Por tanto, debemos convertir los datos de tipo objeto en tipo flotante o entero.
                        <br /> Existen diversos métodos para transformar los datos categóricos en numéricos. Pandas nos proporciona la función replace que permite modificar conjuntos de valores por otros.
                        <py-repl>
                            df = titanic_copia.copy()
                            df['Sex'] = df['Sex'].replace(['female','male'],[0,1])
                            display(df)
                        </py-repl>
                        Sin embargo, un método más cómodo para realizar dicha transformación es usando la función get_dummies() de Pandas.
                        <py-repl>
                            titanic_copia = pd.get_dummies(titanic_copia, columns = ["Sex"], prefix=["Sex"])
                            display(titanic_copia.head())
                        </py-repl>
                        Podemos comprobar que se han añadido 2 nuevas columnas en el conjunto de datos, una para el sexo femenino y otra para el sexo masculino.
                        <h4>Combinación de columnas</h4>
                        Otra técnica de la transformación de datos consiste en combinar la información de las columnas del conjunto de datos. En nuestro caso, tenemos que las columnas SibSp y Parch representan la siguiente información:
                        <ul>
                            <li>SibSp, representa si el pasajero viaja con hermanos o no.</li>
                            <li>Parch, representa si el pasajero viaja con padres o no.</li>
                        </ul>
                        Dicha información podemos unificarla creando una nueva columna que represente el tamaño de la familia del pasajero.
                        <br />Para ello, creamos una nueva columna en nuestro conjunto de datos con dicha información y eliminaremos la información de las columnas SibSp y Parch.
                        <py-repl>
                            titanic_copia['FamilySize'] = titanic_copia['SibSp'] + titanic_copia['Parch'] + 1
                            titanic_copia.drop(['SibSp','Parch'], axis=1, inplace=True)
                            display(titanic_copia.head())
                        </py-repl>
                        <h4>Contenedores de datos</h4>
                        Otra técnica que podemos aplicar es crear contenedores de datos, es decir, si tenemos datos con numeros valores podemos agruparlos en datos más sencillos como por ejemplo, en rangos de valores.
                        <br />En nuestro caso, podemos crear contenedores de datos para las columnas Age y Fare. Para realizar este proceso, Pandas nos aporte la función cut.
                        <py-repl>
                            # Contenedor de datos de la columna Age
                            titanic_copia['Age_bin'] = pd.cut(titanic_copia['Age'], bins=[0,12,20,40,120], labels=['Children','Teenage','Adult','Elder'])
                            # Contenedor de datos de la columna Fare
                            titanic_copia['Fare_bin'] = pd.cut(titanic_copia['Fare'], bins=[0,7.91,14.45,31,120], labels=['Low_fare','median_fare','Average_fare','high_fare'])
                            # Eliminamos las columnas Age y Fare
                            titanic_copia.drop(['Age','Fare'], axis=1, inplace=True)
                            # Mostramos el conjunto de datos
                            display(titanic_copia.head())
                        </py-repl>
                        Si observamos el conjunto de datos, podemos comprobar que se han creado 2 nuevas columnas pero que contienen datos categóricos. Como vimos anteriormente con el Sexo, podemos transformar el tipo de objeto usando la función get_dummies
                        <py-repl>
                            titanic_copia = pd.get_dummies(titanic_copia, columns = ["Age_bin", 'Fare_bin'], prefix=["Age_type", 'Fare_type'])
                            display(titanic_copia.head())
                        </py-repl>
                        <h3>Análisis de los datos</h3>
                        Una vez aplicado el proceso de transformación de datos, podemos comenzar a analizar los datos. Para realizar este proceso, aplicaremos principalmente la libreria seaborn y analizaremos los resultados obtenidos enfocándonos a la meta establecida para el conjunto de datos
                        <br />Nota: Este proceso se puede hacer en este momento o al final, al obtener conclusiones, debido a que se emplearán los métodos que se mostrarán a continuación para las conclusiones
                        Lo primero que debemos hacer es detectar la característica principal del conjunto de datos. En nuestro caso, la característica principal será la información proporcionada por la columna Survived, pues su información indica si el pasajero sobrevivió o no.
                        <h4>Datos balanceados y no desbalanceados</h4>
                        Al analizar un conjunto de datos, es importante comprobar si los datos están balanceados o desbalanceados. En el código a continuación veremos como comprobarlo:
                        <py-repl>
                            import seaborn as sns
                            import matplotlib.pyplot as plt
                            sns.countplot( x='Survived', data=titanic_copia)
                            plt.title('Distribucion de supervivientes/muertos del titanic')
                            display(plt)
                        </py-repl>
                        <em>Explicación del código</em>
                        <ul>
                            <li>Usamos la función countplot de la librería seaborn para generar un gráfico de barras.</li>
                            <li>x='Survived' significa que el eje X del gráfico representará la información correspondiente a la columna 'Survived'.</li>
                            <li>data=titanic_copia significa que se empleará para generar el gráfico el conjunto de datos titanic_copia</li>
                            <li>Usamos la función title de la librería matplotlib para añadir un título al gráfico generado.</li>
                        </ul>
                        Si analizamos el gráfico anterior, podemos comprobar que los datos están desbalanceados, habiendo más pasajeros muertos que pasajeros vivos.
                        <h4>Gráficos categóricos</h4>
                        <br />En el código a continuación, vamos a analizar aquellos pasajeros con mayor tasa de supervicencia en función del sexo femenino.
                        <py-repl>
                            import seaborn as sns
                            import matplotlib.pyplot as plt
                            # Generamos el grafico usando la funcion catplot de seaborn
                            sns.catplot(x='Sex_female', hue='Survived', kind='count', data=titanic_copia)
                            # Mostramos el grafico generado
                            display(plt)
                            # Limpiamos el grafico generado
                            plt.clf()
                        </py-repl>
                        Si observamos la gráfica, podemos comprobar que sobrevivieron más mujeres que hombres en el hundimiento del titanic. 
                        <br />Otro estudio que podemos realizar es si el tipo de clase del pasajero influyó en la supervivencia de los pasajeros.
                        <py-repl>
                            import seaborn as sns
                            import matplotlib.pyplot as plt
                            # Generamos el grafico usando la funcion catplot de seaborn
                            sns.catplot(x='Pclass', hue='Survived', kind='count', data=titanic_copia)
                            # Mostramos el grafico generado
                            display(plt)
                            # Limpiamos el grafico generado
                            plt.clf()
                        </py-repl>
                        Si observamos el gráfico, podemos comprobar que tuvieron mayor tasa de supervivencia las personas con una clase de tiquet más elevado. 
                        Otros posibles análisis a realizar, sería analizar si influye en la tasa de supervivencia la edad de la persona, si iban en familia o no, etc.
                    </p>

                </div>
                <!--Paso 3: Algoritmos-->
                <div id="Algoritmos" style="display:none">
                    <h1>Paso 3. Algoritmos</h1>
                    En esta sección veremos distintos métodos de implementar algoritmos de minería de datos, así como distintas métricas para analizar el los resultados obtenidos en dichos algoritmos.
                    <br />Para este tutorial, se utilizará el conjunto de datos iris.
                    <h2>Obtención del conjunto de datos</h2>
                    En el código a continuación, obtendremos el conjunto de datos iris
                    <py-repl>
                        import pandas as pd
                        from pyodide.http import open_url
                        iris = pd.read_csv(open_url("https://raw.githubusercontent.com/p72losaj/Datasets/main/iris.data"),
                        header=None,
                        sep=",",
                        names = ["sepal length","sepal width","petal length","petal width","Species"])
                        display(iris.head())
                    </py-repl>
                    <h2>Preprocesamiento de datos</h2>
                    Antes de comenzar a diseñar los distintos algoritmos de minería de datos, es recomendable realizar un análisis y, en caso de ser necesario, un preprocesamiento de los datos del dataset.
                    <br />Nota: Se establecerá como meta final predecir las especies de iris
                    <br />Si visualizamos el conjunto de datos, podemos comprobar que no existe información innecesaria, es decir, no podemos eliminar ninguna columna del conjunto de datos.
                    <br />Este proceso lo veremos en el código a continuación:
                    <py-repl>
                        import seaborn as sns
                        import matplotlib.pyplot as plt
                        display('Comprobando la existencia de valores perdidos en el conjunto de datos iris'); display(iris.isnull().sum())
                        display('Analizando el numero de elementos por clase del conjunto de datos'); display(iris['Species'].value_counts())
                        # Comprobamos la existencia de datos atipicos
                        sns.boxplot(x=iris['Species'],y=iris['sepal length']);plt.title('Datos atipicos sepal length');display(plt);plt.clf()
                        sns.boxplot(x=iris['Species'],y=iris['sepal width']); plt.title('Datos atipicos sepal width'); display(plt);plt.clf()
                        sns.boxplot(x=iris['Species'],y=iris['petal length']);plt.title('Datos atipicos petal length');display(plt);plt.clf()
                        sns.boxplot(x=iris['Species'],y=iris['petal width']); plt.title('Datos atipicos petal width'); display(plt);plt.clf()
                        # Comprobamos la existencia de datos duplicados
                        display('Numero de datos duplicados: ', iris.duplicated(keep='first').sum())
                    </py-repl>
                    A partir del código anterior, observamos la siguiente información:
                    <ul>
                        <li>El conjunto de datos no contiene valores perdidos.</li>
                        <li>El conjunto de datos está balanceado, es decir, las clases de iris tienen el mismo número de elementos.</li>
                        <li>Aunque existen valores atípicos, la cantidad de los mismos es muy reducida. En nuestro tutorial, vamos a optar por dejar dichos datos atípicos.</li>
                        <li>
                            Aunque existen datos duplicados, comprobamos que sólo existen 3 filas duplicadas. Por este motivo, y teniendo en cuenta que si eliminamos dichos datos se puede producir un desbalanceamiento de los datos
                            , para este tutorial no vamos a eliminar dichos datos duplicados.
                        </li>
                        <li>Si visualizamos la información del dataset, podemos comprobar que sólo tiene datos no numéricos el atributo de clase. En este caso, no vamos a transformar los datos categóricos en datos numéricos.</li>
                    </ul>
                    <h2>Modelado de los datos</h2>
                    Una vez realizado el preprocesamiento de los datos, debemos separar la información objetivo (característica de clase) del resto de información. Esto se conoce como modelado de datos.
                    <br />En el código a continuación, aplicaremos la biblioteca scikit-learn para modelar los datos:
                    <py-repl>
                        # Dividimos la información
                        X = iris.drop(['Species'], axis=1); Y = iris['Species']
                        # Comprobamos que se ha dividido correctamente la informacion
                        display(X.head()); display(Y.head())
                    </py-repl>

                    <br />A continuación, podemos entrenar y realizar pruebas de los datos mediante distintos algoritmos de minería de datos. Tenemos 2 formas de hacerlo
                    <ol>
                        <li>Entrenamiento y test del mismo conjunto de datos: Existe riesgo de sobreentrenamiento de los datos.</li>
                        <li>
                            Dividiendo el conjunto de datos en datos de entrenamiento y datos de test: Permite asegurarnos de no usar las mismas observaciones en todos los datos
                            de entrenamiento.
                            <br /> Inconveniente: Las puntuaciones de precisión del conjunto de pruebas pueden variar según las observaciones que haya en el conjunto.

                        </li>
                    </ol>
                    Un método común utilizado para evitar el problema del sobreentrenamiento de los datos es la validación cruzada.
                    La validación cruzada es una técnica utilizada para evaluar los resultados de un análisis estadístico y garantizar que son independientes de la partición
                    entre datos de entrenamiento y de prueba.
                    <br />
                    Nota: Al diseñar los algoritmos de mineria de datos, es recomendable realizar una búsqueda de los mejores parámetros de diseño del algoritmo. Para ello, un método eficaz es usando GridSearchCV.
                    <br />
                    GridSearchCV es un método de python que usa la técnica de Validación Cruzada para darte los mejores hiperparametros de un algoritmo de Aprendizaje Automático.
                    <br />Para este tutorial, diseñaremos los algoritmos buscando los mejores hiperparámetros
                    <h2>Árbol de Decisión</h2>
                    En esta sección, utilizaremos el conjunto de datos iris para diseñar el siguiente algoritmo de minería de datos: Árbol de decisión.
                    <br />Además, estudiaremos distintas técnicas de modelado de los datos, con el objetivo de obtener los mejores resultados posibles.
                    <br />Un árbol de decisión es un algoritmo de aprendizaje supervisado no paramétrico, que se utiliza tanto para tareas de clasificación como de regresión.
                    <br />Para el diseño de los árboles de decisión, podemos usar 2 criterios:
                    <ul>
                        <li>Índice gini: El índice Gini es la probabilidad de que una variable no clasifique correctamente si se eligió al azar.</li>
                        <li>Entropía: La entropía es la medida de la impureza de un conjunto de datos; alternativamente, podemos pensar en esto como la medida de incertidumbre del grupo.</li>
                    </ul>
                    <h3>Diseño del método GridSearchCV</h3>
                    En el código a continuacuón, diseñaremos el método gridSearchCV usando como modelo un árbol de decisión:
                    <py-repl>
                        # Importamos las librerias necesarias
                        from sklearn.tree import DecisionTreeClassifier
                        from sklearn.model_selection import GridSearchCV
                        # Establecemos los parametros para gridsearchcv
                        param_dict = {
                        "criterion": ["gini", "entropy"],
                        "max_depth": range(1,10),
                        "min_samples_split": range(1,10),
                        "min_samples_leaf": range(1,5)
                        }
                        # Encontramos el mejor hiperparámetro usando gridsearchCV y como modelo un árbol de decisión
                        grid = GridSearchCV(DecisionTreeClassifier(), param_grid = param_dict, cv=5, verbose=1, n_jobs=-1)
                    </py-repl>
                    Nota: Tener en cuenta que en código que veremos a continuación, se producirá error al ejecutar el código si no habéis ejecutado el código anterior.
                    <h3>Mismo conjunto de datos</h3>
                    En el código a continuación
                    <py-repl>
                        # Entrenamos el modelo
                        grid.fit(X,Y)
                        # Obtenemos los mejores parámetros
                        display('Mejores parámetros: ', grid.best_params_)
                        # Obtenemos el mejor resultado
                        display('Mejor resultado: ', grid.best_score_)
                    </py-repl>
                    <ul>
                        <li>Se ha aplicado como criterio el índice gini.</li>
                        <li>La profundidad máxima del árbol es 3.</li>
                        <li>El mínimo de hojas de muestras ha sido 1.</li>
                        <li>La división mínima de muestras ha sido 3.</li>
                        <li>El mejor resultado obtenido ha sido del 97.33%</li>
                    </ul>
                    <h3>Dividiendo el conjunto de datos en datos de entrenamiento y de test</h3>
                    En el código a continuación, repetiremos el proceso anterior pero aplicando la división del conjunto de datos en datos de entrenamiento y de test.
                    <py-repl>
                        # Dividimos los datos en datos de entrenamiento y de test, dividiendo el dataset en 40% datos de test y 60% datos de entrenamiento
                        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=5)
                        # Entrenamos el modelo
                        grid.fit(X_train,y_train)
                        # Visualizamos los mejores hiperparámetros del árbol de decisión
                        display('Mejores parámetros: ', grid.best_params_)
                        # Visualizamos la mejor puntuación obtenida
                        display('Mejor resultado: ', grid.best_score_)
                    </py-repl>

                    <ul>
                        <li>Se ha aplicado como criterio el índice gini.</li>
                        <li>La profundidad máxima del árbol es 2.</li>
                        <li>El mínimo de hojas de muestras ha sido 1.</li>
                        <li>La división mínima de muestras ha sido 2.</li>
                        <li>El mejor resultado obtenido ha sido del 95.55%</li>
                    </ul>
                    <br />
                    Dado que los resultados obtenidos al dividir el conjunto de datos son bastante buenos, vamos a aplicar la división del conjunto de datos
                    para el resto del tutorial.

                    <h3>Normalización de los datos</h3>
                    La normalización es una técnica común a la hora de solucionar el problema de tener diferentes escalas en los valores de un atributo.
                    <br />El objetivo de esta técnica es que todos los valores de un atributo estén comprendidos en el intervalo [0,1].
                    <br />En el código a continuación, repetiremos el estudio del código anterior aplicando la normalización de los datos
                    <py-repl>
                        from sklearn import preprocessing
                        from sklearn.model_selection train_test_split
                        # Normalizamos los datos
                        X_normalized = preprocessing.normalize(X)
                        # Dividimos los datos en datos de entrenamiento y de test, dividiendo el dataset en 40% datos de test y 60% datos de entrenamiento
                        X_train, X_test, y_train, y_test = train_test_split(X_normalized, Y, test_size=0.4, random_state=5)
                        # Entrenamos el modelo
                        grid.fit(X_train,y_train)
                        # Visualizamos los mejores hiperparámetros del árbol de decisión
                        display('Mejores hiperparámetros: ', grid.best_params_)
                        # Visualizamos la mejor puntuación obtenida
                        display('Mejor resultado: ', grid.best_score_)
                    </py-repl>

                    <ul>
                        <li>Se ha aplicado como criterio el índice gini.</li>
                        <li>La profundidad máxima del árbol es 6.</li>
                        <li>El mínimo de hojas de muestras ha sido 2.</li>
                        <li>La división mínima de muestras ha sido 5.</li>
                        <li>El mejor resultado obtenido ha sido del 94.44%</li>
                    </ul>

                    <h3>Estandarización de los datos</h3>
                    En el código a continuación, aplicaremos el estudio anterior utilizando la estandarización de los datos:
                    <py-repl>
                        from sklearn import preprocessing
                        from sklearn.model_selection import train_test_split
                        # Estandarizamos los datos
                        X_estandarized = preprocessing.scale(X)
                        # Dividimos los datos en datos de entrenamiento y de test, dividiendo el dataset en 40% datos de test y 60% datos de entrenamiento
                        X_train, X_test, y_train, y_test = train_test_split(X_estandarized, Y, test_size=0.4, random_state=5)
                        # Entrenamos el modelo
                        grid.fit(X_train,y_train)
                        # Visualizamos los mejores hiperparámetros del árbol de decisión
                        display('Mejores hiperparámetros: ', grid.best_params_)
                        # Visualizamos la mejor puntuación obtenida
                        display('Mejor resultado: ', grid.best_score_)
                    </py-repl>
                    Si ejecutamos el código anterior, obtenemos los siguientes resultados:
                    <ul>
                        <li>Se ha aplicado como criterio el índice gini.</li>
                        <li>La profundidad máxima del árbol es 2.</li>
                        <li>El mínimo de hojas de muestras ha sido 1.</li>
                        <li>La división mínima de muestras ha sido 2.</li>
                        <li>El mejor resultado obtenido ha sido del 95.55%</li>
                    </ul>
                    <h2>KNN</h2>
                    <p>
                        Se definen los algoritmos KNN como algoritmos de aprendizaje supervisado simples y fáciles de aplicar que pueden ser utilizados para resolver problemas
                        <br />de clasificación y de regresión.
                        <br />
                        En el siguiente código diseñaremos el modelo KNN usando gridsearchCV
                        <py-repl>
                            # Importamos las librerias necesarias
                            from sklearn.model_selection import GridSearchCV
                            from sklearn.neighbors import KNeighborsClassifier
                            # Establecemos los parametros para gridsearchcv
                            param_dict = { "n_neighbors": range(3,10) }
                            # Encontramos el mejor hiperparámetro usando gridsearchCV
                            grid = GridSearchCV(KNeighborsClassifier(), param_grid = param_dict, cv=5, verbose=1, n_jobs=-1)
                        </py-repl>
                        A continuación, repetimos los procesos anteriores al conjunto de datos original (dividiendo el dataset en datos de entrenamiento y de test)
                        <br />
                        Nota: Tened en cuenta que si no ejecutáis el código anterior os dará error al ejecutar las sentencias de código a continuación:
                        <br />
                        En el siguiente código, aplicamos el algoritmo KNN sobre los datos originales
                        <py-repl>
                            # Importamos las librerias necesarias
                            from sklearn.model_selection import train_test_split
                            # Dividimos los datos en datos de entrenamiento y de test, dividiendo el dataset en 40% datos de test y 60% datos de entrenamiento
                            X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=5)
                            # Entrenamos el modelo
                            grid.fit(X_train,y_train)
                            # Visualizamos los mejores hiperparámetros del árbol de decisión
                            display('Mejores hiperparámetros: ', grid.best_params_)
                            # Visualizamos la mejor puntuación obtenida
                            display('Mejor resultado: ', grid.best_score_)
                        </py-repl>
                        Si ejecutamos el código, obtenemos un resultado del 96.66% con el hiperparámetro n_neighbors=3
                        <br />
                        En el siguiente código, realizamos el estudio aplicando normalización de los datos
                        <py-repl>
                            # Importamos las librerias necesarias
                            from sklearn.model_selection import train_test_split
                            from sklearn import preprocessing
                            # Normalizamos los datos
                            X_normalized = preprocessing.normalize(X)
                            # Dividimos los datos en datos de entrenamiento y de test, dividiendo el dataset en 40% datos de test y 60% datos de entrenamiento
                            X_train, X_test, y_train, y_test = train_test_split(X_normalized, Y, test_size=0.4, random_state=5)
                            # Entrenamos el modelo
                            grid.fit(X_train,y_train)
                            # Visualizamos los mejores hiperparámetros del árbol de decisión
                            display('Mejores hiperparámetros: ', grid.best_params_)
                            # Visualizamos la mejor puntuación obtenida
                            display('Mejor resultado: ', grid.best_score_)
                        </py-repl>
                        Al ejecutar el código, hemos obtenido como mejor resultado un 98.89% con n_neighbors=7.
                        <br />
                        A continuación, repetimos el proceso aplicando estandarización de los datos
                        <py-repl>
                            # Importamos las librerias necesarias
                            from sklearn.model_selection import train_test_split
                            from sklearn import preprocessing
                            # Estandarizamos los datos
                            X_estandarized = preprocessing.scale(X)
                            # Dividimos los datos en datos de entrenamiento y de test, dividiendo el dataset en 40% datos de test y 60% datos de entrenamiento
                            X_train, X_test, y_train, y_test = train_test_split(X_estandarized, Y, test_size=0.4, random_state=5)
                            # Entrenamos el modelo
                            grid.fit(X_train,y_train)
                            # Visualizamos los mejores hiperparámetros del árbol de decisión
                            display('Mejores hiperparámetros: ', grid.best_params_)
                            # Visualizamos la mejor puntuación obtenida
                            display('Mejor resultado: ', grid.best_score_)
                        </py-repl>
                        Al ejecutar el código, hemos obtenido un resultado del 96.66% con n_neighbors=5
                    </p>

                    <h2>Árbol de Decisión Aleatorio</h2>
                    <br />Otro tipo de árbol de decisión que podemos generar, es un árbol de decisión aleatorio.
                    <br /> Un árbol de decisión aleatorio (RandomForest) es una combinación de árboles predictores tal que cada árbol depende de los valores
                    <br /> de un vector aleatorio probado independientemente y con la misma distribución para cada uno de estos.
                    <br />En el código a continuación, diseñaremos un árbol de decisión aleatorio usando GridSearchCV
                    <br />Aviso: Debido a la búsqueda de hiperparámetros, puede producirse un fallo de página al emplear la búsqueda GridSearchCV.
                    <py-repl>
                        # Importamos las librerias necesarias
                        from sklearn.ensemble import RandomForestClassifier
                        from sklearn.model_selection import GridSearchCV
                        # Establecemos los parametros para gridsearchcv
                        param_dict = {
                        "criterion": ["gini","entropy", 'log_loss'],
                        "max_depth": range(1,3),
                        "min_samples_split": range(2,5),
                        "min_samples_leaf": range(1,3)
                        }
                        # Encontramos el mejor hiperparámetro usando gridsearchCV
                        grid = GridSearchCV(RandomForestClassifier(),param_grid = param_dict,cv=5,verbose=1,n_jobs=-1)
                    </py-repl>
                    En el código a continuación, realizaremos un estudio sobre los datos originales tras su división en 60% datos de entrenamiento y 40% datos de test.
                    <py-repl>
                        # Importamos las librerias necesarias
                        from sklearn.model_selection import train_test_split
                        # Dividimos los datos en datos de entrenamiento y de test, dividiendo el dataset en 40% datos de test y 60% datos de entrenamiento
                        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=5)
                        # Entrenamos el modelo
                        grid.fit(X_train,y_train)
                        # Visualizamos los mejores hiperparámetros del árbol de decisión
                        display('Mejores hiperparámetros: ', grid.best_params_)
                        # Visualizamos la mejor puntuación obtenida
                        display('Mejor resultado: ', grid.best_score_)
                    </py-repl>
                    Si ejecutamos el código anterior, obtenemos los siguientes resultados:
                    <ul>
                        <li>Se ha aplicado como criterio la entropía</li>
                        <li>La profundidad máxima del árbol es 2.</li>
                        <li>El mínimo de hojas de muestras ha sido 1.</li>
                        <li>La división mínima de muestras ha sido 4.</li>
                        <li>El mejor resultado obtenido ha sido del 96%</li>
                    </ul>
                    En el código a continuación, realizamos el estudio de los datos tras aplicar normalización
                    <py-repl>
                        # Importamos las librerias necesarias
                        from sklearn.model_selection import train_test_split
                        from sklearn import preprocessing
                        # Normalizamos los datos
                        X_normalized = preprocessing.normalize(X)
                        # Dividimos los datos en datos de entrenamiento y de test, dividiendo el dataset en 40% datos de test y 60% datos de entrenamiento
                        X_train, X_test, y_train, y_test = train_test_split(X_normalized, Y, test_size=0.4, random_state=5)
                        # Entrenamos el modelo
                        grid.fit(X_train,y_train)
                        # Visualizamos los mejores hiperparámetros del árbol de decisión
                        display('Mejores hiperparámetros: ', grid.best_params_)
                        # Visualizamos la mejor puntuación obtenida
                        display('Mejor resultado: ', grid.best_score_)
                    </py-repl>
                    Si ejecutamos el código anterior, obtenemos los siguientes resultados:
                    <ul>
                        <li>Se ha aplicado como criterio el índice gini</li>
                        <li>La profundidad máxima del árbol es 2.</li>
                        <li>El mínimo de hojas de muestras ha sido 2.</li>
                        <li>La división mínima de muestras ha sido 2.</li>
                        <li>El mejor resultado obtenido ha sido del 93.33%</li>
                    </ul>
                    En el código a continuación, realizamos el estudio de los datos aplicando la estandarización de los datos
                    <py-repl>
                        # Importamos las librerias necesarias
                        from sklearn.model_selection import train_test_split
                        from sklearn import preprocessing
                        # Estandarizamos los datos
                        X_estandarized = preprocessing.scale(X)
                        # Dividimos los datos en datos de entrenamiento y de test, dividiendo el dataset en 40% datos de test y 60% datos de entrenamiento
                        X_train, X_test, y_train, y_test = train_test_split(X_estandarized, Y, test_size=0.4, random_state=5)
                        # Entrenamos el modelo
                        grid.fit(X_train,y_train)
                        # Visualizamos los mejores hiperparámetros del árbol de decisión
                        display('Mejores hiperparámetros: ', grid.best_params_)
                        # Visualizamos la mejor puntuación obtenida
                        display('Mejor resultado: ', grid.best_score_)
                    </py-repl>
                    Si ejecutamos el código anterior, obtenemos los siguientes resultados:
                    <ul>
                        <li>Se ha aplicado como criterio el índice gini</li>
                        <li>La profundidad máxima del árbol es 2.</li>
                        <li>El mínimo de hojas de muestras ha sido 1.</li>
                        <li>La división mínima de muestras ha sido 4.</li>
                        <li>El mejor resultado obtenido ha sido del 96.66%</li>
                    </ul>
                    <h2>SVM</h2>
                    <p>
                        Un algoritmo SVM (Máquinas de Vectores Soporte) es un algoritmo de clasificación y regresión basado en el concepto del hiperplano.
                        <br />
                        En el código anterior, diseñaremos un modelo SVM de tipo rbf o lineal, y realizaremos una búsqueda de los mejores hiperparámetros mediante GridSearchCV.
                        <py-repl>
                            # Importamos las librerias necesarias
                            from sklearn import svm
                            from sklearn.model_selection import GridSearchCV
                            # Establecemos los parametros para gridsearchcv
                            param_dict = {
                            "kernel": ['linear','rbf'],
                            "C": [0.01, 0.1, 1, 10, 100, 1000, 10000],
                            "gamma": [0.02, 0.2, 2, 20, 200, 2000, 20000, "auto"]
                            }
                            # Encontramos el mejor hiperparámetro usando gridsearchC
                            grid = GridSearchCV(svm.SVC(),param_grid = param_dict,cv=5,verbose=1,n_jobs=-1)
                        </py-repl>
                        En el código a continuación, realizaremos un estudio sobre los datos originales tras su visión en 60% datos de entrenamiento y 40% datos de test
                        <py-repl>
                            # Importamos las librerias necesarias
                            from sklearn.model_selection import train_test_split
                            # Dividimos los datos en datos de entrenamiento y de test, dividiendo el dataset en 40% datos de test y 60% datos de entrenamiento
                            X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=5)
                            # Entrenamos el modelo
                            grid.fit(X_train,y_train)
                            # Visualizamos los mejores hiperparámetros del árbol de decisión
                            display('Mejores hiperparámetros: ', grid.best_params_)
                            # Visualizamos la mejor puntuación obtenida
                            display('Mejor resultado: ', grid.best_score_)
                        </py-repl>
                        Si ejecutamos el código anterior, obtenemos los siguientes resultados.
                        <ol>
                            <li>Se ha aplicado un algoritmo SVC lineal.</li>
                            <li>Valor del parámetro C: 1.</li>
                            <li>Valor del parámetro gamma: 0.02</li>
                            <li>Se ha obtenido un resultado del 98.89%</li>
                        </ol>
                        A continuación, realizamos el estudio de los datos usando normalización:
                        <py-repl>
                            # Importamos las librerias necesarias
                            from sklearn.model_selection import train_test_split
                            from sklearn import preprocessing
                            # Normalizamos los datos
                            X_normalized = preprocessing.normalize(X)
                            # Dividimos los datos en datos de entrenamiento y de test, dividiendo el dataset en 40% datos de test y 60% datos de entrenamiento
                            X_train, X_test, y_train, y_test = train_test_split(X_normalized, Y, test_size=0.4, random_state=5)
                            # Entrenamos el modelo
                            grid.fit(X_train,y_train)
                            # Visualizamos los mejores hiperparámetros del árbol de decisión
                            display('Mejores hiperparámetros: ', grid.best_params_)
                            # Visualizamos la mejor puntuación obtenida
                            display('Mejor resultado: ', grid.best_score_)
                        </py-repl>
                        Si ejecutamos el código anterior, obtenemos los siguientes resultados.
                        <ol>
                            <li>Se ha aplicado un algoritmo SVC de tipo rbf.</li>
                            <li>Valor del parámetro C: 0.1</li>
                            <li>Valor del parámetro gamma: 200</li>
                            <li>Se ha obtenido un resultado del 98.89%</li>
                        </ol>
                        A continuación, realizamos el estudio de los datos usando estandarización:
                        <py-repl>
                            # Importamos las librerias necesarias
                            from sklearn.model_selection import train_test_split
                            from sklearn import preprocessing
                            # Estandarizamos los datos
                            X_estandarized = preprocessing.scale(X)
                            # Dividimos los datos en datos de entrenamiento y de test, dividiendo el dataset en 40% datos de test y 60% datos de entrenamiento
                            X_train, X_test, y_train, y_test = train_test_split(X_estandarized, Y, test_size=0.4, random_state=5)
                            # Entrenamos el modelo
                            grid.fit(X_train,y_train)
                            # Visualizamos los mejores hiperparámetros del árbol de decisión
                            display('Mejores hiperparámetros: ', grid.best_params_)
                            # Visualizamos la mejor puntuación obtenida
                            display('Mejor resultado: ', grid.best_score_)
                        </py-repl>
                        Si ejecutamos el código anterior, obtenemos los siguientes resultados.
                        <ol>
                            <li>Se ha aplicado un algoritmo SVC de tipo lineal.</li>
                            <li>Valor del parámetro C: 10</li>
                            <li>Valor del parámetro gamma: 0.02</li>
                            <li>Se ha obtenido un resultado del 97.78%</li>
                        </ol>
                    </p>
                </div>
                
                <!--Paso 4: Conclusiones-->
                <div id="conclusiones" style="display:none">
                    <h1>Paso 4. Conclusiones</h1>
                    Este paso consiste en redactar un informe explicativo de los resultados obtenidos durante todo el proceso de minería de datos realizado, las decisiones
                    tomadas durante el mismo y las conclusiones finales.
                </div> 

                <!--Ejercicios propuestos-->
                <div id="EjerciciosPropuestos" style="display:none">
                    <h1>Ejercicios propuestos</h1>
                    <p>
                        A continuación, se popondrán un conjunto de ejercicios a resolver por el usuario.
                        <h3>Ejercicio 1. Aplica las etapas del proceso de minería de datos al siguiente conjunto de datos</h3>
                        <py-repl>
                            url_dataset = https://raw.githubusercontent.com/p72losaj/Datasets/main/wine.data
                            col_names = ['class', 'Alcohol', 'Malic_acid', 'Ash', 'Alcalinity_of_ash', 'Magnesium', 'Total_phenols','Flavanoids', 'Nonflavanoid_phenols', 'Proanthocyanins', 'Color_intensity', 'Hue', 'OD280/OD315','Proline']
                        </py-repl>
                        <h3>Ejercicio 2. Aplica las etapas del proceso de minería de datos al siguiente conjunto de datos</h3>
                        <py-repl>
                            url_dataset = https://raw.githubusercontent.com/p72losaj/Datasets/main/diabetes.csv
                            col_names = ['preg', 'plas','pres','skin','insu','mass','pedi','age','class']
                        </py-repl>
                        <h3>Ejercicio 3. Aplica las etapas del proceso de minería de datos al siguiente conjunto de datos</h3>
                        <py-repl>
                            url_dataset = https://raw.githubusercontent.com/p72losaj/Datasets/main/abalone.data
                            col_names = ['Sex', 'Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight', 'Rings/Class']
                        </py-repl>
                        <h3>Ejercicio 4. Aplica las etapas del proceso de minería de datos al siguiente conjunto de datos</h3>
                        <py-repl>
                            url_dataset = https://raw.githubusercontent.com/p72losaj/Datasets/main/glass.arff
                        </py-repl>
                        <h3>Ejercicio 5. Aplica las etapas del proceso de minería de datos al siguiente conjunto de datos</h3>
                        <py-repl>
                            url_dataset = https://raw.githubusercontent.com/p72losaj/Datasets/main/Dry_Bean_Dataset.arff
                        </py-repl>
                        <h3>Ejercicio 6. Aplica las etapas del proceso de minería de datos al siguiente conjunto de datos</h3>
                        <py-repl>
                            url_dataset = https://raw.githubusercontent.com/p72losaj/Datasets/main/car.data
                            col_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']
                        </py-repl>
                        <h3>Ejercicio 7. Aplica las etapas del proceso de minería de datos al siguiente conjunto de datos</h3>
                        <py-repl>
                            url_dataset = https://raw.githubusercontent.com/p72losaj/Datasets/main/zoo.data
                            col_names = ['animal_name', 'hair', 'feathers', 'eggs', 'milk', 'airzorne', 'aquatic', 'predator', 'toothed', 'backbone', 'breathes', 'venomous', 'fins', 'legs', 'tail', 'domestic', 'catsize', 'class']
                        </py-repl>
                    </p>
                </div>
            </div>
        </div>
    </div>

</body>
</html>

